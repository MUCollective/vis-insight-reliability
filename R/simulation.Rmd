---
title: "B-H for incentive calculation"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: readable

---

```{r setup, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(dplyr)
library(purrr)
library(furrr)
library(beepr)
library(tidyr)
library(forcats)
library(patchwork)
library(glue)
library(ggridges)
```

We want to show that our decision-making incentives encourage the rational participant to avoid excess false positives in a multiple-comparison scenario. The simulation is in two parts: first, we compare the maximum payout attenable via different decision-making strategies. These strategies include:

- Do-nothing (no multiple-comparison correction)
- Bonferroni correction (controlling FWER)
- Benjamini-Hochberg (controlling FDR)
- Only selecting one with the smallest $p$-value
- Selecting none
- Selecting random x

Purpose of this document: Using Benjamini-Hochberg to justify the FP penalty being 19. The simulation will give us the expected total payout under B-H and FP penalty = 19, so we can create a grid of $\alpha$'s and $P(null)$'s and see which $alpha$ gives the highest payout. 


Executive summary: 

- Under B-H, $\alpha = 0.03, 0.04$ around $P(null) = 0.5$ gives the highest expected total payout
- Without any correction strategy, $\alpha = 0.01$ (the lowest value simulated) maximizes payout around $P(null) = 0.5$.


## Setup params

We can get the expected number of selections under B-H with a few simulation iterations and take the average. Given that all the parameters, such as $\alpha$, are fixed, the uncertainty comes from the sampling of true $\mu$'s from a Binomial and the random sampling of $p$-value.


- The simulation has `n_iter` of "trials" in our experiment
- For each trial, draw 8 or 12 true $\mu$ (0 or $\mu$) from $Bin(n, 1- P(null))$
- With the $\mu$ and pre-specified $\sigma$ et al., draw 8 or 12 $p$-values 
- Do the B-H and reject hypotheses/regions accordingly
- We get the number of rejections that _should_ happen under B-H, for both 8 and 12 regions.




Once we have the distribution of # of rejections B-H says we should make, we can take the average and get the expected value for number of selections. 

```{r}
alpha <- 0.05    # not using in simulation
K <- 20          # number of data points in a hypothesis/region
p_null <- 0.5    # the bane of my existance 
n_iter <- 1000   # number of iterations in simulation
alphas <- seq(from = 0.001, to = 0.1, by = 0.001)
p_nulls <- 0.5 # ppoints(10)
fp_penalty <- 19
```

## p-value PDF, CDF, invCDF defs

- Definitions of PDF and CDF of the $p$-value from [@hung_behavior_1997]
- Random draws from the $p$-value PDF `rp` is sampled through random draws from the uniform [0, 1] quantile space, then looked up through the inverse CDF function (using `uniroot`). 
- **Limitation**: These functions are dependent on the $\mu$, $\sigma$, $K$ parameters. These parameters are most likely different IRL but we are not using other values yet.


```{r}
# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}

# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu/sigma)
}


# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}

# random sample from PDF of p-value using its invCDF
rp <- function( mu, sigma){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```


## Grid search for the good alpha

Putting the above pieces together, we vary $alpha$ and $P(null)$ but keep $\mu$, $\sigma$ and $K$ the same. 

### Encapsulate simulation function

This is the same code as above

```{r}
finding_payout <- function(alpha, p_null){
  df <- 
    expand_grid(nregions = c(8, 12),
                iter = 1:n_iter) %>%
    split(1:nrow(.)) %>%
    map_dfr(~ bind_cols(.,
                        tibble(mu0 = c(0.9, 1.2, 1.5),
                               sigma = c(3, 8/3, 2.5)))) %>%
    mutate(delta = mu0 / sigma) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    group_by(iter, nregions, delta) %>%
    mutate(
      mu = mu0 * rbinom(nregions, 1, 1 - p_null), 
      sigma = mu0 / delta,
      p_raw = map2_dbl(mu, sigma, ~rp(.x, .y)), 
      p_bh = p.adjust(p_raw, method = "BH"),
      p_bonf = p.adjust(p_raw, method = "bonferroni"),

    ) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method, delta) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - fp_penalty * fp + tn - fn) * 1,
           ineq = tp - fp_penalty * fp - tn + fn,
           power = (tp)/(tp + fn)) # this produces NA's; set to 1?
 
  
df %>%
  group_by(method, nregions, delta) %>%
  summarize(E_pay = mean(pay), 
            E_fdr = mean(fdr), 
            E_ineq = mean(ineq),
            power = mean(power), .groups = "drop")
}

```


### Run

setting seed https://davisvaughan.github.io/furrr/articles/articles/gotchas.html#argument-evaluation

```{r, eval= FALSE}
plan(multisession, workers = 13)
options <- furrr_options(seed = 32)

(sim_df <- expand.grid(
    alpha = seq(0, 0.10, length.out = 3),
    p_null = p_nulls
  ) %>%
    split(1:nrow(.)) %>%
    future_map_dfr(~cbind(.x, finding_payout(.$alpha, .$p_null), row.names = NULL),
                   .options = options, .progress = TRUE)
)

beep()
saveRDS(sim_df, "sim_n1000_alpha_pnull_delta_seed32.rds")
```


```{r}
sim_df <- readRDS("sim_n1000_alpha_pnull_delta_seed32.rds") %>%
  mutate(method = fct_recode(method, Uncorrected = "p_raw", Bonferroni = "p_bonf", `Benjamini-Hochberg` = "p_bh"))
```

### Results

Benjamini-Hochberg gives the highest payout around P(null) = 0.5 at $\alpha = 0.05$

```{r}
sim_df %>% 
  filter(alpha == 0.05) %>%
  ggplot(aes(x = p_null, y = E_pay, color = method)) +
  geom_line() + 
  facet_grid(nregions ~ delta) + 
  labs(title = "Which (multiple comparisons) correction method gives higher payout")
```

Checking the incentives: we pllot the $\alpha$ value where payout is the highest, and see how close it is to 0.05 under different strategies.

```{r}
sim_df %>%
  group_by(alpha, nregions, method, delta) %>% # average out P(null)
  summarize(E_pay_mean = mean(E_pay), .groups = "drop")  %>% 
  group_by(nregions, method, delta) %>%
  slice_max(E_pay_mean)  %>%
  ggplot(aes(x = factor(delta), y = alpha)) + 
  geom_point() + 
  geom_segment(aes(xend = factor(delta), yend = 0)) + 
  geom_hline(aes(yintercept = 0.05), color = "slategray3") +
  facet_grid(nregions ~ method) + 
  theme_minimal() + 
  labs(title = "The alphas that maximizes E[payout] for three deltas and nregions",
       subtitle = "Averaged over a uniform P(null) distribution")
```

### Other explorations


```{r}
sim_df %>%
  ggplot(aes(alpha, E_fdr, color = p_null)) +
  geom_point(alpha = 0.2) + 
  geom_line(data = sim_df %>% filter(p_null == 0.475)) +
  geom_abline(aes(intercept = 0, slope = 1)) + 
  facet_grid(delta ~ method) + 
  scale_color_viridis_c() + 
  theme_minimal() + 
  labs(title = "FDR under different", subtitle = "Diagonal line is alpha = FDR, colored line at P(null) = 0.5")
```


```{r}
sim_df %>%
  ggplot(aes(alpha, power)) + 
  geom_point(aes(color = p_null), alpha = 0.2) + 
  geom_vline(aes(xintercept = 0.05)) +
  geom_hline(aes(yintercept = 0.8)) +
  facet_grid(nregions ~ method + delta) +
  scale_color_viridis_c() + 
  theme_minimal() +
  labs(title="Power is higher without correction", subtitle = "A bunch of NA's for when P(null) is high")
```

## Comparing stimuli data to alternative datasets drawn from the same data generating process

For Section 5.3 Limitations


```{r}
data.sim.12reg <- readRDS(file = "../data/simulated_data_12regions.rds")
data.sim.8reg <- readRDS(file = "../data/simulated_data_8regions.rds")

(df_stimuli <- bind_rows(
  list("12" = data.sim.12reg, "8" = data.sim.8reg), 
  .id = "nregions") %>%
  select(-c(region_idx, effect_size, population, p_h1)) %>%
  mutate(nregions = as.numeric(nregions),
         delta = mean/sd)
)
```




```{r}
stimuli_fn <- function(df_stimuli, alpha){
  df_stimuli %>%
    group_by(nregions, trial, delta) %>%
    mutate(p_raw = 
             map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)) %>%
    mutate(p_bh = p.adjust(p_raw, "BH")) %>%
    select(-data) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p") %>%
    mutate(reject = p < alpha,
           true = mu == 0) %>%
    group_by(trial, nregions, method, delta) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1,
           power = (tp)/(tp + fn)) %>% # this produces NA's; set to 1?
    group_by(nregions, method, delta) %>%
    summarize(E_pay = mean(pay), 
              E_fdr = mean(fdr), 
              power = mean(power), .groups = "drop")
}

```


```{r}
stimuli_alldelta_fn <- function(df_stimuli, alpha){
  df_stimuli %>%
    group_by(nregions, trial) %>%
    mutate(p_raw = 
             map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)) %>%
    mutate(p_bh = p.adjust(p_raw, "BH")) %>%
    select(-data) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p") %>%
    mutate(reject = p < alpha,
           true = mu == 0) %>%
    group_by(trial, nregions, method) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1,
           power = (tp)/(tp + fn)) %>% # this produces NA's; set to 1?
    group_by(nregions, method) %>%
    summarize(across(everything(), mean), .groups = "drop")
}
```


```{r}
stimuli_alpha_df <- expand.grid(alpha = alphas) %>%
  split(1:nrow(.)) %>%
  map_dfr(~ cbind(.x, stimuli_fn(df_stimuli, .$alpha), row.names = NULL))


stimuli_alldelta_df <-  expand.grid(alpha = alphas) %>%
  split(1:nrow(.)) %>%
  map_dfr(~ cbind(.x, stimuli_alldelta_fn(df_stimuli, .$alpha), row.names = NULL))
```

Table of confusion matrix categories from stimuli data, under two strategies



```{r}
(stimuli_fdr_plot_df <- stimuli_alldelta_df %>%
  filter(alpha == 0.05) %>%
  select(-pay) %>% 
  mutate(method = fct_recode(method, Uncorrected = "p_raw",  `Benjamini-Hochberg` = "p_bh"))
)
```





```{r}
(p3 <- sim_df %>% 
  filter(alpha == 0.05 & method != "Bonferroni") %>%
  ggplot(aes(x = E_fdr, y = factor(nregions))) +
  geom_vline(aes(xintercept = 0.05)) +
   geom_point(alpha = 0.4) +
   geom_point(data = stimuli_fdr_plot_df, mapping = aes(x = fdr, y = factor(nregions), color = factor(nregions))) +
   # geom_segment(data = stimuli_fdr_plot_df, mapping = aes(x = ))
  facet_grid(method ~ delta) + 
   # xlim(c(0, 0.1)) +
  labs(title= glue("Black dots: simulated data wuth {n_iter} iterations, colored dots: stimuli"))
)
```


