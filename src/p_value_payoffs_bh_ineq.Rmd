---
title: "B-H for incentive calculation"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(purrr)
library(furrr)
library(beepr)
library(tidyr)
library(forcats)
library(patchwork)
library(glue)
```

We want to show that our decision-making incentives encourage the rational participant to avoid excess false positives in a multiple-comparison scenario. The simulation is in two parts: first, we compare the maximum payout attenable via different decision-making strategies. These strategies include:

- Do-nothing (no multiple-comparison correction)
- Bonferroni correction (controlling FWER)
- Benjamini-Hochberg (controlling FDR)
- Only selecting one with the smallest $p$-value
- Selecting none
- Selecting random x

Purpose of this document: Using Benjamini-Hochberg to justify the FP penalty being 19. The simulation will give us the expected total payout under B-H and FP penalty = 19, so we can create a grid of $\alpha$'s and $P(null)$'s and see which $alpha$ gives the highest payout. 


Executive summary: 

- Under B-H, $\alpha = 0.03, 0.04$ around $P(null) = 0.5$ gives the highest expected total payout
- Without any correction strategy, $\alpha = 0.01$ (the lowest value simulated) maximizes payout around $P(null) = 0.5$.

<!-- Previous iteration [[202011120919 Alpha level and FP penalty explainer]] -->

## Setup params

```{r}
alpha <- 0.05    # not using in simulation
K <- 20          # number of data points in a hypothesis/region

# ACHTUNG: mu/sigma is c(1.2/3, 1.5/2.5, 1.6/2) in stimuli, picking the average here
# TODO: fix mu/sigma
# mu <- 1.5        # sample mean in a hypothesis/region
# sigma <- 2.5     

p_null <- 0.5    # the bane of my existance 
n_iter <- 100   # number of iterations in simulation
alphas <- seq(from = 0.001, to = 0.1, by = 0.001)
p_nulls <- ppoints(10)
fp_penalty <- 19
```

## p-value PDF, CDF, invCDF defs

- Definitions of PDF and CDF of the $p$-value from [@hung_behavior_1997]
- Random draws from the $p$-value PDF `rp` is sampled through random draws from the uniform [0, 1] quantile space, then looked up through the inverse CDF function (using `uniroot`). 
- **Limitation**: These functions are dependent on the $\mu$, $\sigma$, $K$ parameters. These parameters are most likely different IRL but we are not using other values yet.


```{r}
# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}

# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu/sigma)
}


# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}

# random sample from PDF of p-value using its invCDF
rp <- function( mu, sigma){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```


## Expected value of number of selections

We can get the expected number of selections under B-H with a few simulation iterations and take the average. Given that all the parameters, such as $\alpha$, are fixed, the uncertainty comes from the sampling of true $\mu$'s from a Binomial and the random sampling of $p$-value.


- The simulation has `n_iter` of "trials" in our experiment
- For each trial, draw 8 or 12 true $\mu$ (0 or $\mu$) from $Bin(n, 1- P(null))$
- With the $\mu$ and pre-specified $\sigma$ et al., draw 8 or 12 $p$-values 
- Do the B-H and reject hypotheses/regions accordingly
- We get the number of rejections that _should_ happen under B-H, for both 8 and 12 regions.


```{r warning = FALSE}
set.seed(32)

(df <-
  expand_grid(nregions = c(8, 12),
    iter = 1:n_iter) %>%
    split(1:nrow(.)) %>%
    map_dfr(~ bind_cols(.,
                        tibble(mu0 = c(0.9, 1.2, 1.5),
                               sigma = c(3, 8/3, 2.5)))) %>%
    mutate(delta = mu0 / sigma) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    group_by(iter, nregions, delta) %>%
    mutate(
      mu = mu0 * rbinom(nregions, 1, 1 - p_null), 
      sigma = mu0 / delta,
      p_raw = map2_dbl(mu, sigma, ~rp(.x, .y)), 
      p_bh = p.adjust(p_raw, method = "BH"),
    ) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method, delta) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1,
           ineq = tp - 19 * fp -tn + fn,
           power = (tp)/(tp + fn)) # this produces NA's
)
```

```{r}
# df %>% 
#   group_by(method) %>%
#   summarize(mean(fdr))
```


Once we have the distribution of # of rejections B-H says we should make, we can take the average and get the expected value for number of selections. 

```{r}
# (expected_nselect <-
#    df %>%
#    group_by(method, nregions) %>%
#    summarise(E_nselect = mean(tp + fp), .groups = "drop")
# )
```



## Expected total payout

There have been three iterations of expected total payout

1. $E[n_{FP}] = E[n_{reject}] * P(null)$. This is (even more) problematic now that we're using B-H. It can overestimate $n_{FP}$ because if people are ordering hypotheses/regions by $p$-values, they should be making false discoveries at a lower rate than $P(null)$.
2. $E[n_{FP}] = E[n_{reject}] * P(true\vert reject) = E[n_{reject}] * \alpha$. By definition, $P(true \vert reject) = E\left[\frac{n_{true \wedge  reject}}{ n_{reject}} \right]= FDR \leq \alpha$, and we say B-H controls FDR at level $\alpha$. But $\alpha$ here is an upperbound instead of the expected value, see [@benjamini_controlling_1995]. 
  - The problem is, we don't have a good expression for $E[n_{TP}]$. If we write $E[n_{TP}] = E[n_{reject}] * P(\neg true \vert reject)$, we don't have an expression for $P(\neg true \vert reject)$ like $FDR \leq \alpha$. If we just use the joint probability $E[n_{TP}] = n P(\neg true, reject)$, $P(\neg true, reject) =1 - \beta$, but there is no closed-form for power for B-H; [@benjamini_controlling_1995] had to run a simulation. So...
3. Screw it, just use the simulation results. Basically the original simulation.


```{r}
# (expected_payout <- expected_nselect %>%
#   mutate(E_tp = E_nselect * (1 - alpha),
#          E_tn = (nregions - E_nselect) * alpha,
#          E_fp = E_nselect * p_null,
#          E_fn = (nregions - E_nselect) * (1 - p_null),
#          E_payout = E_tp - 19 * E_fp + E_tn - E_fn)
# )

# (expected_payout <- expected_nselect %>%
#   mutate(E_tp = E_nselect * (1 - p_null), 
#          E_tn = (nregions - E_nselect) * p_null,
#          E_fp = E_nselect * p_null,
#          E_fn = (nregions - E_nselect) * (1 - p_null),
#          E_payout = E_tp - 19 * E_fp + E_tn - E_fn)
# )
df %>%
  group_by(method, nregions, delta) %>%
  summarize(E_pay = mean(pay), 
            E_fdr = mean(fdr), 
            E_ineq = mean(ineq),
            power = mean(power), .groups = "drop")
```



## Grid search for the good alpha

Putting the above pieces together, we vary $alpha$ and $P(null)$ but keep $\mu$, $\sigma$ and $K$ the same. 

### Encapsulate simulation function

This is the same code as above

```{r}
finding_payout <- function(alpha, p_null){
  df <- 
    expand_grid(nregions = c(8, 12),
                iter = 1:n_iter) %>%
    split(1:nrow(.)) %>%
    map_dfr(~ bind_cols(.,
                        tibble(mu0 = c(0.9, 1.2, 1.5),
                               sigma = c(3, 8/3, 2.5)))) %>%
    mutate(delta = mu0 / sigma) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    group_by(iter, nregions, delta) %>%
    mutate(
      mu = mu0 * rbinom(nregions, 1, 1 - p_null), 
      sigma = mu0 / delta,
      p_raw = map2_dbl(mu, sigma, ~rp(.x, .y)), 
      p_bh = p.adjust(p_raw, method = "BH"),
    ) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method, delta) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - fp_penalty * fp + tn - fn) * 1,
           ineq = tp - fp_penalty * fp - tn + fn,
           power = (tp)/(tp + fn)) # this produces NA's; set to 1?
 
  
df %>%
  group_by(method, nregions, delta) %>%
  summarize(E_pay = mean(pay), 
            E_fdr = mean(fdr), 
            E_ineq = mean(ineq),
            power = mean(power), .groups = "drop")
}

```


### Run

setting seed https://davisvaughan.github.io/furrr/articles/articles/gotchas.html#argument-evaluation

```{r, eval= FALSE}
plan(multisession, workers = 13)
options <- furrr_options(seed = 32)

(sim_df <- expand.grid(
    alpha = alphas,
    p_null = p_nulls
  ) %>%
    split(1:nrow(.)) %>%
    future_map_dfr(~cbind(.x, finding_payout(.$alpha, .$p_null), row.names = NULL), .options = options)
)

beep()
saveRDS(sim_df, "sim_n100_alpha_pnull_delta_seed32.rds")
# saveRDS(sim_df, "sim_n5000_alpha_pnull.rds")
# saveRDS(sim_df, "sim_n5000_alpha_pnull_delta.rds")
# saveRDS(sim_df, "sim_n5000_alpha_pnull_delta_seed32.rds")
```


```{r}
# sim_df <- readRDS("sim_n5000_alpha_pnull.rds")
sim_df <- readRDS("sim_n100_alpha_pnull_delta_seed32.rds")
```

### Results


See where "Rejecting" gives greater payout than "not rejecting" on the $\alpha$ scale

```{r}
sim_df %>%
  group_by(nregions, method, delta, alpha) %>%
  summarize(E_ineq = mean(E_ineq)) %>%
  ggplot(aes(alpha, E_ineq)) + 
  geom_point(aes(color = delta)) + 
  theme_minimal() + 
  scale_color_viridis_c() +
  facet_grid(nregions ~ method) +
  geom_hline(aes(yintercept = 0))
```



The rest


```{r}
sim_df %>% 
  ggplot(aes(p_null, E_pay, color = alpha)) + 
  geom_point(alpha = 0.2) + 
  geom_line(data = sim_df %>% filter(alpha == 0.05)) + 
  facet_grid(nregions ~ method + delta) + 
  scale_color_viridis_c() + 
  theme_minimal()  +
  labs(title = "Which alpha maximizes payout?", subtitle = "Line drawn at alpha = 0.05")
```


```{r}
sim_df %>% 
  filter(alpha < 0.2 & method != "p_raw") %>%
  ggplot(aes(p_null, E_pay, color = alpha)) + 
  geom_point(alpha = 0.2) + 
  geom_line(data = sim_df %>% filter(alpha == 0.05 & method != "p_raw")) + 
  facet_grid(nregions ~ method + delta) + 
  scale_color_viridis_c() + 
  theme_minimal()  +
  labs(title = "Which alpha maximizes payout?", subtitle = "Line drawn at alpha = 0.05")
```
  
```{r}
sim_df %>% 
  filter(alpha < 0.05) %>%
  ggplot(aes(alpha, E_pay, color = p_null)) + 
  # geom_point(alpha = 0.2) +
  geom_line(aes(group = p_null), alpha = 0.5)  +
  # geom_line(data = sim_df %>% filter(p_null %in% c(0.025, 0.475, 0.975)), aes(group = p_null)) + 
  facet_grid(nregions ~ method + delta) + 
  scale_color_viridis_c() + 
  theme_minimal() +
  labs(title = "Which alpha maximizes payout?", subtitle = "alpha = 0.02 ~ 0.04")
```

```{r}
sim_df %>% 
  group_by(nregions, delta, method, alpha) %>%
  summarize(E_pay = mean(E_pay)) %>%
  ggplot(aes(alpha, E_pay, color = method)) + 
  geom_point(aes(group = p_null), alpha = 0.5)  +
  facet_grid(nregions ~ delta) + 
  scale_color_viridis_d() + 
  theme_minimal()  + 
  labs(title = "At which alpha is B-H better than RAW")
```
```{r}

sim_df %>%
  group_by(alpha, nregions, method, delta) %>% # average out P(null)
  summarize(E_pay_mean = mean(E_pay), .groups = "drop") %>%
 ggplot(aes(x = alpha, y = E_pay_mean, color = factor(delta))) + 
  geom_point() + 
  facet_grid(nregions ~ method) + 
  scale_color_viridis_d() + 
  theme_minimal() 

```

```{r}
sim_df %>%
  # filter(p_null %in% c(0.475, 0.525)) %>%
  group_by(alpha, nregions, method, delta) %>% # average out P(null)
  summarize(E_pay_mean = mean(E_pay), .groups = "drop")  %>% 
  group_by(nregions, method, delta) %>%
  slice_max(E_pay_mean)  %>%
  ggplot(aes(x = factor(delta), y = alpha)) + 
  geom_point() + 
  geom_segment(aes(xend = factor(delta), yend = 0)) + 
  geom_hline(aes(yintercept = 0.05), color = "slategray3") +
  facet_grid(nregions ~ method) + 
  theme_minimal() + 
  labs(title = "The alphas that maximizes E[payout] for three (delta=mu/sigma)'s and nregions",
       subtitle = "Averaged over a uniform P(null) distribution")
```




```{r}
sim_df %>%
  ggplot(aes(alpha, E_fdr, color = p_null)) +
  geom_point(alpha = 0.2) + 
  geom_line(data = sim_df %>% filter(p_null == 0.475)) +
  geom_abline(aes(intercept = 0, slope = 1)) + 
  facet_grid(. ~ method + delta) + 
  scale_color_viridis_c() + 
  theme_minimal() + 
  labs(title = "FDR under B-H and raw strategy", subtitle = "Diagonal line is alpha = FDR, colored line at P(null) = 0.5")
```


```{r}
sim_df %>%
  ggplot(aes(alpha, power)) + 
  geom_point(aes(color = p_null), alpha = 0.2) + 
  geom_vline(aes(xintercept = 0.05)) +
  geom_hline(aes(yintercept = 0.8)) +
  facet_grid(nregions ~ method + delta) +
  scale_color_viridis_c() + 
  theme_minimal() +
  labs(title="Power is higher without correction", subtitle = "A bunch of NA's for when P(null) is high")
```

## Comparing simulation to experimental stimuli data


```{r}
data.sim.12reg <- readRDS(file = "../data/simulated_data_12regions.rds")
data.sim.8reg <- readRDS(file = "../data/simulated_data_8regions.rds")

(df_stimuli <- bind_rows(
  list("12" = data.sim.12reg, "8" = data.sim.8reg), 
  .id = "nregions") %>%
  select(-c(region_idx, effect_size, population, p_h1)) %>%
  mutate(nregions = as.numeric(nregions),
         delta = mean/sd)
)
```

```{r}
stimuli_fn <- function(df_stimuli, alpha){
  df_stimuli %>%
    group_by(nregions, trial, delta) %>%
    mutate(p_raw = 
             map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)) %>%
    mutate(p_bh = p.adjust(p_raw, "BH")) %>%
    select(-data) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p") %>%
    mutate(reject = p < alpha,
           true = mu == 0) %>%
    group_by(trial, nregions, method, delta) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1,
           power = (tp)/(tp + fn)) %>% # this produces NA's; set to 1?
    group_by(nregions, method, delta) %>%
    summarize(E_pay = mean(pay), 
              E_fdr = mean(fdr), 
              power = mean(power), .groups = "drop")
}

```


```{r}
stimuli_alldelta_fn <- function(df_stimuli, alpha){
  df_stimuli %>%
    group_by(nregions, trial) %>%
    mutate(p_raw = 
             map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)) %>%
    mutate(p_bh = p.adjust(p_raw, "BH")) %>%
    select(-data) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p") %>%
    mutate(reject = p < alpha,
           true = mu == 0) %>%
    group_by(trial, nregions, method) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1,
           power = (tp)/(tp + fn)) %>% # this produces NA's; set to 1?
    group_by(nregions, method) %>%
    summarize(E_pay = mean(pay), 
              E_fdr = mean(fdr), 
              power = mean(power), .groups = "drop")
}
```


```{r}
stimuli_alpha_df <- expand.grid(alpha = alphas) %>%
  split(1:nrow(.)) %>%
  map_dfr(~ cbind(.x, stimuli_fn(df_stimuli, .$alpha), row.names = NULL))


stimuli_alldelta_df <-  expand.grid(alpha = alphas) %>%
  split(1:nrow(.)) %>%
  map_dfr(~ cbind(.x, stimuli_alldelta_fn(df_stimuli, .$alpha), row.names = NULL))
```

```{r}
(stimuli_fdr_plot_df <- stimuli_alldelta_df %>%
  filter(alpha == 0.05) %>%
  select(-E_pay)
)
  # ggplot(aes(x = E_fdr, y = nregions)) +
  # geom_point() + 
  # geom_vline(aes(xintercept = 0.05))  +
  # xlim(c(0, .2)) + 
  # labs(title = "Stimulis data, without grouping by delta")
```



```{r}
(p1 <- stimuli_alpha_df %>%
  ggplot(aes(alpha, E_pay)) + 
  geom_point() + 
   geom_vline(aes(xintercept = 0.05), color = "red") + 
  facet_grid(nregions ~ method + delta) +
  scale_color_viridis_c() + 
  theme_minimal() +
  labs(title = "t-test on experiment stimuli", subtitle = "P(null) = 0.5")
)
```

```{r}
(p2 <- sim_df %>% 
  filter(p_null < 0.46 & p_null > 0.45) %>%
  ggplot(aes(alpha, E_pay)) + 
  # geom_point(alpha = 0.2) +
  geom_point()  +
  # geom_line(data = sim_df %>% filter(p_null %in% c(0.025, 0.475, 0.975)), aes(group = p_null)) + 
  facet_grid(nregions ~ method + delta) + 
  scale_color_viridis_c() + 
  theme_minimal() +
  labs(title = "Simulation results")
)
```

```{r}
# p1 + p2 
  bind_rows("stimuli" = stimuli_alpha_df, "simulated" = sim_df %>% filter(p_null < 0.55 & p_null > 0.45), .id = "source") %>%
  ggplot(aes(x = alpha, y = E_pay)) + 
  geom_point(aes(color = source), alpha = 0.3)  + 
  facet_grid(nregions ~ method + delta) + 
  theme_minimal()
```

## Normative FDR for different number of hypotheses


For the discussion section


```{r}
(p3 <- sim_df %>% 
  filter(alpha == 0.05) %>%
  # filter(method == "p_bh") %>%
  group_by(method, delta, nregions) %>%
  summarize(mean_fdr = mean(E_fdr),
            ci = 1.96 * sd(E_fdr)/sqrt(n())) %>%
  ggplot(aes(x = mean_fdr, y = factor(nregions))) +
  geom_vline(aes(xintercept = 0.05)) +
  geom_pointrange(aes(xmin = mean_fdr - ci, xmax = mean_fdr + ci)) +
   geom_point(data = stimuli_fdr_plot_df, mapping = aes(x = E_fdr, y = factor(nregions), color = factor(nregions))) + 
  facet_grid(method ~ delta) + 
   xlim(c(0, 0.1)) +
  labs(title= glue("Dot and whisker: simulated data, {n_iter} iterations, dot: stimuli"))
)
```



```{r}
(p4 <- 
stimuli_alpha_df %>% 
  filter(alpha == 0.05) %>%
  # filter(method == "p_bh") %>%
  ggplot(aes(x = E_fdr, y = factor(nregions))) +
  geom_point() + 
  geom_vline(aes(xintercept = 0.05)) +
  facet_grid(method ~ delta) +
  labs(title = "With experimental stimuli, equiv to one iteration")
)
  
```
```{r}
p3 /p4
```

