---
title: "Exploration of variance in low-probability binomial events"
author: "Abhraneel Sarma"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggdist)

# set up the global theme
theme_set(theme_minimal())
```



## Introduction

In this document I outline the considerations involved in designing an experiment which tests whether users, in an exploratory visual data analysis scenario, perform multiple comparisons corrections when they are comparing multiple hypotheses. A typical null hypothesis significance test is performed at an $\alpha = 0.05$ which suggests that, one in every 20 "discoveries" using this procedure will be a false discovery. However, in a multiple comparisons procedure, the probability of finding at least one false positive increases as the number of comparisons increases.


### A note on simulations

In this document, we will sample p-values directly, instead of simulating data and then calculating a p-value (for a particular null hypothesis). This speeds up the process (as we do not have to sample random data), and allows greater control over the generated stimuli.

### The distribution of p-value

*When the null is true*: the distribution of the p-value is uniform if the null is true. 

*When the alternative hypothesis is true*: Given by [@Hung et al.
]The following functions are relevant to determine the distribution of the p-value when the alternative hypothesis is true. Note that the function `rp` allows us to draw random samples under the assumption that the alternative hypothesis is true.

```{r}
# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}
# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu/sigma)
}
# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}
# random sample from PDF of p-value using its invCDF
rp <- function(mu, sigma, K){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```

Next, let us verify the distribution of p-values obtained using our simulation. When the null is true, the distribution should be uniform:

```{r}
mu = 3
sd = 10
m = c(1, 4, 8, 12, 20, 30, 50)
ntrials = 1000
p_null = 0.7
alpha = 0.05

set.seed(1234)

data.sim.pvals = crossing(m, p0 = p_null, alpha, trial = 1:ntrials) |>
  mutate(
    `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x))
  ) |>
  unnest(c(`mu > 0`)) |>
  mutate(p_uncorrected = map2_dbl(`mu > 0`, m, ~ ifelse(.x == 0, runif(1, 0, 1), rp(mu, sd, .y))))

data.sim.pvals |>
  ungroup() |>
  filter(`mu > 0` == 0) |>
  ggplot() +
  geom_histogram(aes(p_uncorrected), boundary = 0, colour = "#ffffff", binwidth = 0.05) +
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))
```

Let's also look at the distribution of p-values when the alternative hypothesis is true:

```{r}
data.sim.pvals |>
  ungroup() |>
  filter(`mu > 0` == 1) |>
  ggplot() +
  geom_histogram(aes(p_uncorrected), boundary = 0, colour = "#ffffff", binwidth = 0.05) +
  scale_x_continuous(breaks = seq(0, 1, by = 0.2))
```


### The probability of a False Positive under multiple comparisons

To understand how the probability of finding at least one false positive changes, let us look at a few different scenarios. We will set `r ntrials = 1000` and `r m = c(1, 4, 8, 12, 20, 30, 50)` where `m` indicates the number of comparisons we are making.

First, we look at the probability of finding at least one false positive, given the null hypothesis is always true. We first simulate the data, and then plot it. We can also plot the theoretical estimate $(1 - (1 - \alpha)^{\pi_0m}$ to ensure our calculations are correct:

```{r}
set.seed(1234)

crossing(m, p0 = 1, alpha = 0.05, trial = 1:ntrials) |>
  mutate(
    `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x))
  ) |>
  unnest(c(`mu > 0`)) |>
  mutate(p = map2_dbl(`mu > 0`, m, ~ ifelse(.x == 0, runif(1, 0, 1), rp(3, 10, .y)))) |> 
  mutate(reject_null = p < alpha) |> # reject or not?
  group_by(m, p0, trial) |>
  mutate(TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
            FP = as.integer(!`mu > 0` & reject_null),
            TN = as.integer(!`mu > 0` & !reject_null),
            FN = as.integer(`mu > 0` & !reject_null),
  ) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) |> # calculates total number of TP, TN, FP, FN for each trial
  # because \pi_0 = 1, here, FDR will always be equal to 1
  # thus correcting for FDR is the same as correcting for FWER
  summarise(FWER = mean(FP > 0), .groups = "drop") |> # estimates Pr(FP) across the set of trial
  mutate(`theoretical estimate` = 1 - 0.95^m) |>
  pivot_longer(cols = c(FWER, `theoretical estimate`)) |>
  ggplot() +
  geom_point(aes(y = value, x = m, color = name)) +
  geom_line(aes(y = value, x = m, color = name))
```

However, the probability of the null being true being equal to one is unrealistic. This can change the probability of finding at least one false positive:

```{r}
data.sim.pvals |> 
  mutate(reject_null = p_uncorrected < alpha) |> 
  group_by(m, p0, trial) |>
  mutate(TP = as.integer(`mu > 0` & reject_null),
            FP = as.integer(!`mu > 0` & reject_null),
            TN = as.integer(!`mu > 0` & !reject_null),
            FN = as.integer(`mu > 0` & !reject_null),
  ) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) |> 
  summarise(FWER = mean(FP > 0), .groups = "drop") |> 
  mutate(`theoretical estimate` = 1 - 0.95^(m*p0)) |>
  pivot_longer(cols = c(FWER, `theoretical estimate`)) |>
  ggplot() +
  geom_point(aes(y = value, x = m, color = name)) +
  geom_line(aes(y = value, x = m, color = name))
```

### Multiple comparisons correction procedure

There exists multiple comparisons corrections procedures such as Bonferroni correction and the Benjamini-Hochberg correction which allows us to control FDR at $\alpha = 0.05$:

```{r fig.height = 4, fig.width = 8}
set.seed(1234)
data.m_compare.pvals = crossing(m, p0 = 0.7, alpha = 0.05, trial = 1:ntrials) |>
  mutate(
    `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x))
  ) |>
  unnest(c(`mu > 0`)) |>
  mutate(
    p_uncorrected = map2_dbl(`mu > 0`, m, ~ ifelse(.x == 0, runif(1, 0, 1), rp(3, 10, .y))),
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni"),
  ) |>
  pivot_longer(cols = starts_with("p_"), names_prefix = "p_", names_to = "method", values_to = "p_val") |>
  group_by(method, m, p0, alpha, trial) |>
  mutate(reject_null = p_val < alpha,
          TP = as.integer(`mu > 0` & reject_null),
          FP = as.integer(!`mu > 0` & reject_null),
          TN = as.integer(!`mu > 0` & !reject_null),
          FN = as.integer(`mu > 0` & !reject_null)) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

p1 = data.m_compare.pvals |> 
  summarise(FWER = mean(FP > 0), .groups = "drop") |> 
  ggplot() +
  geom_point(aes(y = FWER, x = m, color = method)) +
  geom_line(aes(y = FWER, x = m, color = method)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1))

p2 = data.m_compare.pvals |> 
  mutate(FDR = ifelse((FP + TP) > 0, FP/(FP + TP), 0)) |>
  summarise(FDR = mean(FDR), .groups = "drop") |> 
  ggplot() +
  geom_point(aes(y = FDR, x = m, color = method)) +
  geom_line(aes(y = FDR, x = m, color = method))

cowplot::plot_grid(p1, p2, ncol = 2)
```
From the plots above, we see that while Bonferroni corrects for the Family-Wise error rate, Benjamini-Hochberg corrects for the False Discorvery Rate.

However, in scenarios where the number of comparisons being performed is small, we may not see sufficiently large FDR as the number of instances with $p < 0.05$ maybe itself be small. I refer to these as `low probability binomial events`

### Low probability binomial events

By this, I am referring to a binomial random variable with small values of p (i.e. close to zero). 
Consider a scenario where an event occurs successfully with a probability `p`. In a single trial, a participant attempts the event `m` times (which is varied). Over a set of 100 trials, we measure the number of successful events, by measuring a `score`, and explore the variance:

```{r fig.height = 7, fig.width = 12}
m = 2^c(1:8)
trials = c(20, 30, 40, 50, 100)
prob = c(0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5)
n = 100

set.seed(100)

simulations = crossing(m, trials, prob, n = 1:n) |>
  mutate(
    event = pmap(list(m, trials, prob), ~ rbinom(..2, ..1, ..3)),
    score = map_dbl(event, ~ mean(.))/m,
    trials = factor(trials)
  )

simulations |>
  group_by(m, trials, prob) |>
  summarise(score = list(score), .groups = "keep") |>
  median_qi(score) |>
  ggplot() +
  geom_pointinterval(aes(x = m, y = score, ymin = .lower, ymax = .upper), position = "dodge") +
  geom_hline(aes(yintercept = prob)) +
  geom_hline(aes(yintercept = 0), size = 0.1, colour = "#efefef") +
  geom_hline(aes(yintercept = 2*prob), size = 0.1, colour = "#efefef") +
  scale_x_log10(breaks = m) +
  scale_color_brewer(palette = "OrRd") +
  facet_grid(prob ~ trials, scales = "free") +
  theme_minimal()
```
Based on the graphs above, here's what occurs:

- in all cases, we observe that the average number of successful events is approximately `p`, which is as expected
- however, for low probability events and small number of attempts (i.e. small m), we observe a lot of variance

Another way of visualising the same dataset:

```{r, fig.height = 6, fig.width = 10}
simulations |>
  mutate(n = factor(n)) |>
  ggplot(aes(y = score, x = m, group = n)) +
  geom_line(alpha = 0.1) +
  geom_hline(aes(yintercept = prob)) +
  geom_hline(aes(yintercept = 0), size = 0.1, colour = "#efefef") +
  geom_hline(aes(yintercept = 2*prob), size = 0.1, colour = "#efefef") +
  scale_x_log10(breaks = m) +
  scale_color_brewer(palette = "OrRd") +
  facet_grid(prob ~ trials, scales = "free") +
  theme_minimal()
```

This has implications for our study design---to test the multiple comparisons problem in exploratory visual data analysis. Consider that NHST without multiple comparisons correction is being performed at a particular $\alpha$ level. The number of comparisons is given by `m`, and the $\alpha$ level is given by `p`. Assuming that the null is true, the probability of a False Positive is thus `p`. Thus, if we ask participants to perform NHST, we would observe similar variance in the average number of False Positives.

Thus, to reliably measure False positives, we want to set the number of potential False Positives sufficiently high. We can do so by using a higher value for $\alpha$ and $m$, s.t. $\alpha \in \{0.2, 0.25, 0.3\}$ and $m = \{8, 12, 16, 20, 24\}$. We don't explore for values of m greater than 24 because it is not possible to show that many graphs on a screen.

**Constraints:**
As described previously, we want set the number of potential False Positives sufficiently high. In other words, we want $\pi_0 \in \{0.5, 0.6, 0.7, 0.8, 0.9\}$. Note that at the lower end of this combination of values (for example, $\pi_0 = 0.5$ and $m = 8$), we should still expect to see a lot of variance as the maximum number of FP is 4, and the expected number of FP is ~1 depending on the value of $\alpha$.

## Variables which will influence the outcome

There are several variables which may impact the number of correct decisions:

- number of comparisons being made
- probability of the null being true
- effect size
- incentive structure

Below, we explore how varying each of these values will impact the number of correct decisions as well as the payoff.

## Effect of P(null)

In the following simulation, we will specify different values of $\pi_0$ to see how that impacts participants False Discovery Rates and payoff based on an incentive structure where {TP: 1, TN: 1, FP: $(1/\alpha - 1)$, FN: -1}

```{r, eval = FALSE}
mean = 3
sd = 10
m = c(10, 20, 30, 40, 50, 100)
ntrials = 1000
p_null = seq(0.5, 0.9, by = 0.1)
alpha = c(0.2, 0.25, 0.3)

set.seed(1234)
data.sim.pvals = crossing(m, p0 = p_null, alpha, trial = 1:ntrials) |>
  mutate(
    `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x))
  ) |>
  unnest(c(`mu > 0`)) |>
  mutate(p_uncorrected = map2_dbl(`mu > 0`, m, ~ ifelse(.x == 0, runif(1, 0, 1), rp(mean, sd, .y))))

# saveRDS(data.sim.pvals, "../data/sim-study-2/data.sim.pvals.rds")
```


```{r, eval = FALSE}
data.sim.pvals = readRDS("../data/sim-study-2/data.sim.pvals.rds")

data.sim.conf_matrix = data.sim.pvals |>
  group_by(m, p0, alpha, trial) |>
  mutate(
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni"),
    p_reject_none = 1,
    p_reject_all = 0
  ) |>
  pivot_longer(cols = starts_with("p_"), names_prefix = "p_", names_to = "method", values_to = "p_val") |>
  group_by(method, m, p0, alpha, trial) |>
  mutate(
    reject_null = p_val < alpha,
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

saveRDS(data.sim.conf_matrix, "../data/sim-study-2/data.sim.conf_matrix.rds")
```


```{r, fig.height = 7, fig.width = 12}
data.sim.conf_matrix = readRDS("../data/sim-study-2/data.sim.conf_matrix.rds") |>
  filter(method != "reject_all") |>
  mutate(
    FDR = ifelse(TP + FP > 0, FP / (TP + FP), 0),
    payoff = TP + TN - FN - (1/alpha - 1)*FP
  )

data.sim.conf_matrix |>
  mean_qi(FDR) |>
  ggplot() +
  geom_pointinterval(aes(x = alpha, y = FDR, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  geom_line(aes(alpha, FDR, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(m ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.2, 0.3, by = 0.05))
```


```{r, fig.height = 7, fig.width = 12}
data.sim.conf_matrix |>
  mean_qi(payoff) |>
  ggplot() +
  geom_pointinterval(aes(x = alpha, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  geom_line(aes(alpha, payoff, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(m ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.2, 0.3, by = 0.05))
```

The graphs above suggest that, if we want to be able to observe a difference between the use of a multiple comparisons correction strategy such as BH over an uncorrected strategy, we need to be asking people to perform 20 to 30 comparisions at a $pi_0$ of at least 0.7 and $\alpha = 0.25$

## The impact of effect size

One thing we did not vary in our previous simulation was the relative values of the mean and standard deviation (i.e. effect size). This plays a role in the number of TP and FN as it controls the distribution of the p-value when the alternative hypothesis is true. Next, we can examine what happens if the effect size varies. To impose some constraints on the number of variables, we will fiz $\pi_0 \in \{0.7, 0.8\}$, $m = 20$ and $\alpha = 0.25$

```{r, eval = FALSE}
mu = c(1, 2, 3, 4, 5)
sd = 10
m = 20
ntrials = 1000
p_null = seq(0.7, 0.8, by = 0.1)
alpha = 0.25

set.seed(1234)
data.sim.effect_size_varying.pvals = crossing(m, p0 = p_null, alpha, mu, sd, trial = 1:ntrials) |>
  mutate(
    `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x)),
    effect_size = mean/sd
  ) |>
  unnest(c(`mu > 0`)) |>
  mutate(p_uncorrected = pmap_dbl(list(`mu > 0`, m, mu, sd), ~ ifelse(..1 == 0, runif(1, 0, 1), rp(..3, ..4, ..2))))

data.sim.effect_size_varying.outcomes = data.sim.effect_size_varying.pvals |>
  group_by(m, p0, alpha, effect_size, trial) |>
  mutate(
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni"),
    p_reject_none = 1
  ) |>
  pivot_longer(cols = starts_with("p_"), names_prefix = "p_", names_to = "method", values_to = "p_val") |>
  group_by(method, m, p0, alpha, effect_size, trial) |>
  mutate(
    reject_null = p_val < alpha,
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

saveRDS(data.sim.effect_size_varying.outcomes, "../data/sim-study-2/data.sim.varying_outcomes.rds")
```

We then re-create the plot which visualises median point estimates and error bars for each strategy

```{r, fig.height = 3, fig.width = 12}
data.sim.effect_size_varying.outcomes = readRDS("../data/sim-study-2/data.sim.varying_outcomes.rds") |>
  mutate(
    FDR = ifelse(TP + FP > 0, FP / (TP + FP), 0),
    payoff = TP + TN - FN - (1/alpha - 1)*FP
  )

data.sim.effect_size_varying.outcomes |>
  mean_qi(FDR) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = FDR, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1))
```

```{r, fig.height = 3, fig.width = 12}
data.sim.effect_size_varying.outcomes |>
  median_qi(payoff) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1))
```


## The impact of varying the payoff structure

The final variable that we can manipulate, which will impact participants' behavior is the incentive matrix. The incentive matrix is associated with a specific reward for each of a True Positive, True Negative, False Positive, False Negative:

```{r}
estimate_average_payoff = function(.data, base, rTP = 1, rTN = 1, rFP = 1, rFN = 1, scale = 10) {
  .data |>
    mutate(
      FDR = ifelse(TP + FP > 0, FP / (TP + FP), 0),
      payoff = base + (rTP*TP + rTN*TN + rFN*FN + rFP*FP) / scale
    ) |>
    summarise(
      TP = mean(TP),
      TN = mean(TN),
      FP = mean(FP),
      FN = mean(FN),
      .lower = mean(payoff) + qnorm(0.1)*sd(payoff),
      .upper = mean(payoff) + qnorm(0.9)*sd(payoff),
      payoff = mean(payoff),
      .groups = "drop"
    )
}
```

Below, we show the payoff from a commonly used incentive matrix for TP/TN/FP/FN: 1, 1, -($\frac{1}{\alpha} - 1$), -1, with a baseline pay of 5 and the reward being scaled by 10:

```{r fig.height = 3, fig.width = 9}
estimate_average_payoff(data.sim.effect_size_varying.outcomes, 5, 1, 1, -3, -1, 10) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1)) +
  coord_cartesian(y = c(0, 10))
```

There appears to be very little distinction between all the correction procedures. Let us look at the FDR, as well as at individual TP/TN/FP/FN rates to better understand what's going on:

```{r fig.height = 8, fig.width = 12}
p1 = data.sim.effect_size_varying.outcomes |>
  pivot_longer(c(TP:FN), names_to = "outcome", values_to = "frequency") |>
  group_by(method, m, p0, alpha, effect_size, outcome) |>
  mean_qi(frequency, .width = 0.8) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = frequency, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.05)) +
  facet_grid(outcome ~ p0, scales = "free_y")

p2 = data.sim.effect_size_varying.outcomes |>
  group_by(method, m, p0, alpha, effect_size, trial) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)  |>
  mutate(FDR = ifelse((FP + TP), FP/(FP + TP), 0)) |>
  mean_qi(FDR, .width = 0.8) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = FDR, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.05)) +
  facet_grid(. ~ p0, scales = "free_y")

cowplot::plot_grid(p1, p2, ncol = 1, rel_heights = c(5, 2))
```

First, we should ignore extremely small effect sizes as the signal is not strong enough to be detected empirically. Additionally, for small to medium effect sizes i.e. $d \geq 0.3$, what we observe is that a multiple comparisions procedure such as BH results in very low FP (as does bonferroni) at the expense of some TP (when compared to the uncorrected strategy); on the other hand, it also results in less FN (when compared to Bonferroni) and more TN (when compared to the uncorrected strategy).

In traditional NHST, False Discovery Rates are controlled at $\alpha = 0.05$, which implies that False Positives are 19 times as costly as a True Positive. As Benjamini-Hochberg is a False Discovery Rate correction procedure, it makes sense to more strongly penalise FP. This leads to an incentive matrix of 1/1/-19/-1:

```{r fig.height = 3, fig.width = 9}
estimate_average_payoff(data.sim.effect_size_varying.outcomes, 10, 1, 1, -19, -1, 10) |>
  mutate(payoff = ifelse(TP == 0, payoff - 10, payoff)) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1)) +
  coord_cartesian(y = c(-0, 15))
```

Further, as experiments are usually conducted with 80% power for detecting an effect, this dictates that a False Negative is four times as costly as a True Positive. However, the relationship between a TP and a TN is not clear, but the ratio of True Negatives to True Positives appears to be $\frac{\pi_0}{(1 - \pi_0)}$. We can try to visualise the payoff based on this outcome matrix: 2.33/1/-19/-1
In addition, we include a further penalty of -10 if there are no positives identified


```{r fig.height = 3, fig.width = 9}
estimate_average_payoff(data.sim.effect_size_varying.outcomes, 10, 3, 1, -9, -1, 10) |>
  mutate(payoff = ifelse(TP == 0, payoff - 10, payoff)) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1)) +
  coord_cartesian(y = c(-10, 20))
```

Other explorations:

```{r fig.height = 3, fig.width = 9}
estimate_average_payoff(data.sim.effect_size_varying.outcomes, 10, 5, 1, -15, -1, 10) |>
  mutate(payoff = ifelse(TP == 0, payoff - 10, payoff)) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1)) +
  coord_cartesian(y = c(-10, 20))

estimate_average_payoff(data.sim.effect_size_varying.outcomes, 10, 10, 1, -30, -1, 10) |>
  mutate(payoff = ifelse(TP == 0, payoff - 10, payoff)) |>
  ggplot() +
  geom_pointinterval(aes(x = effect_size, y = payoff, ymin = .lower, ymax = .upper, colour = method), position = position_dodge(width = 0.02)) +
  facet_grid(. ~ p0, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.5, by = 0.1)) # +
  # coord_cartesian(y = c(-10, 20))
```

## Experiment Stimuli

Based on the above explorations, we anticipate that $m \in \{12, 16, 20\}$, $\pi_0 = 0.7$ and $\delta = 0.4$ would likely allow us to detect differences between the use of a multiple comparisons strategy as opposed to an uncorrected strategy.

The code below shows that the following combination of `seed` and `m` leads to highest effect size:
- seed = 6748 and m = 12
- seed = 6319 and m = 16
- seed = 8533 and m = 20
We should use this for generating our stimuli.

```{r, eval = FALSE}
mu = 4
sd = 10
m = c(12, 16, 20)
ntrials = 10
p_null = 0.7
alpha = 0.25

simulate_pval = function(.seed) {
  set.seed(.seed)
  
  crossing(m, p0 = p_null, alpha, trial = 1:ntrials) |>
    mutate(
      `mu > 0` = map2(m, p0, ~ sample(c(rep(0, round(.y * .x)), rep(1, round((1 - .y) * .x))), .x))
    ) |>
    unnest(c(`mu > 0`)) |>
    mutate(p_uncorrected = map2_dbl(`mu > 0`, m, ~ ifelse(.x == 0, runif(1, 0, 1), rp(mu, sd, .y))))
}

df.seed.stimuli = tibble(.seed = 1:1e4) |>
  mutate(.sim = map(.seed, simulate_pval)) |>
  unnest(.sim) |>
  group_by(.seed, m, trial) |>
  mutate( p_BH = p.adjust(p_uncorrected, "BH") ) |>
  pivot_longer(cols = starts_with("p_"), names_prefix = "p_", names_to = "method", values_to = "p_val") |>
  group_by(.seed, method, m, trial) |>
  mutate(
    reject_null = p_val < alpha,
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) |>
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) |>
  mutate( payoff = (TP - 3*FP)*5 + (TN - FN) ) |>
  group_by(.seed, method, m)

df.seed.stimuli |>
  summarise(mean = mean(payoff), sd = sd(payoff), .groups = "keep") |>
  pivot_wider(names_from = "method", values_from = c(mean, sd), names_sep = ".") |>
  mutate( effect_size = (mean.BH - mean.uncorrected) / sqrt((9*(sd.BH^2 + sd.uncorrected^2)) / 18) ) |>
  group_by(.seed, m) |>
  # mutate(avg.effect_size = mean(effect_size)) |>
  arrange(desc(effect_size))
```

```{r}
df.seed.stimuli |>
  filter(
    (.seed == 6748 & m == 12) | (.seed == 6319 & m == 16) | (.seed == 8533 & m == 20)
  ) |>
  mutate(FDR = ifelse((FP + TP), FP/(FP + TP), 0)) |>
  pivot_longer(c(payoff, FDR), names_to = "outcome") |>
  group_by(.seed, method, m, outcome) |>
  ggplot(aes(x = factor(m), y = value, colour = method)) +
  geom_point(alpha = 0.5, position = position_dodge(width = 0.4)) +
  stat_pointinterval(.width = 0.95, position = position_dodge(width = 0.2)) +
  facet_grid(outcome ~ ., scales = "free") + 
  theme(panel.spacing = unit(2, "lines"))
```

```{r}
x1 = rnorm(20, 0, 10)

t.test(x1 + 4, alternative = "greater")$p.value
```

```{r}
map_dbl(seq(3, 5, by = 0.01), ~ t.test(x1 + ., alternative = "greater")$p.value)
```

