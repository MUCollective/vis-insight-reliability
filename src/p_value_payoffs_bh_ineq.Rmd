---
title: "B-H for incentive calculation"
output: html_document
---

```{r setup, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(tidybayes)
library(purrr)
library(tidyr)
library(gganimate)
library(ggrepel)
library(modelr)
library(glue)
library(forcats)
```

Purpose of this document: Using Benjamini-Hochberg to justify the FP penalty being 19. The simulation will give us the expected total payout under B-H and FP penalty = 19, so we can create a grid of $\alpha$'s and see which $alpha$ gives the highest payout. 


<!-- Previous iteration [[202011120919 Alpha level and FP penalty explainer]] -->

## Setup params

```{r}
alpha <- 0.05    # not using in simulation
K <- 20          # number of data points in a hypothesis/region

# ACHTUNG: mu/sd is c(1.2/3, 1.5/2.5, 1.6/2) in stimuli, picking the average here
mu <- 1.5        # sample mean in a hypothesis/region
sigma <- 2.5     

p_null <- 0.5    # the bane of my existance 
n_iter <- 100   # number of iterations in simulation
```

## p-value PDF, CDF, invCDF defs

- Definitions of PDF and CDF of the $p$-value from [@hung_behavior_1997]
- Random draws from the $p$-value PDF `rp` is sampled through random draws from the uniform [0, 1] quantile space, then looked up through the inverse CDF function (using `uniroot`). 
- **Limitation**: These functions are dependent on the $\mu$, $\sigma$, $K$ parameters. These parameters are most likely different IRL but we are not using other values yet.


```{r}
# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}

# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu / sigma)
}


# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}

# random sample from PDF of p-value using its invCDF
rp <- function( mu, sigma){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```


## Expected value of number of selections

We can get the expected number of selections under B-H with a few simulation iterations and take the average. Given that all the parameters, such as $\alpha$, are fixed, the uncertainty comes from the sampling of true $\mu$'s from a Binomial and the random sampling of $p$-value.


- The simulation has `n_iter` of "trials" in our experiment
- For each trial, draw 8 or 12 true $\mu$ (0 or $\mu$) from $Bin(n, 1- P(null))$
- With the $\mu$ and pre-specified $\sigma$ et al., draw 8 or 12 $p$-values 
- Do the B-H and reject hypotheses/regions accordingly
- We get the number of rejections that _should_ happen under B-H, for both 8 and 12 regions.


```{r warning = FALSE}
(df <-
  expand.grid(
  nregions = c(8, 12),
  iter = 1:n_iter
) %>%
  uncount(nregions, .remove = FALSE, .id = "panel") %>%
  group_by(iter, nregions) %>%
  mutate(
    mu = mu * rbinom(nregions, 1, 1 - p_null), 
    p_raw = map_dbl(mu, ~rp(.x, sigma)), 
    p_bh = p.adjust(p_raw, method = "BH"),
  )  %>%
  pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
  mutate(
    true = mu == 0, # null hypothesis being true
    reject = p < alpha
  ) %>%
  group_by(iter, nregions, method) %>%
  summarize( tp = sum(!true & reject),
             fp = sum(true & reject),
             tn = sum(true & !reject),
             fn = sum(!true & !reject), 
             .groups = "drop_last") %>%
  mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
         pay = (tp - 19 * fp + tn - fn) * 1) 
)
```

```{r}
df %>% 
  group_by(method) %>%
  summarize(mean(fdr))
```


Once we have the distribution of # of rejections B-H says we should make, we can take the average and get the expected value for number of selections. 

```{r}
(expected_nselect <-
   df %>%
   group_by(method, nregions) %>%
   summarise(E_nselect = mean(tp + fp), .groups = "drop")
)
```



## Expected total payout

There have been three iterations of expected total payout

1. $E[n_{FP}] = E[n_{reject}] * P(null)$. This is (even more) problematic now that we're using B-H. It can overestimate $n_{FP}$ because if people are ordering hypotheses/regions by $p$-values, they should be making false discoveries at a lower rate than $P(null)$.
2. $E[n_{FP}] = E[n_{reject}] * P(true\vert reject) = E[n_{reject}] * \alpha$. By definition, $P(true \vert reject) = E\left[\frac{n_{true \wedge  reject}}{ n_{reject}} \right]= FDR \leq \alpha$, and we say B-H controls FDR at level $\alpha$. But $\alpha$ here is an upperbound instead of the expected value, see [@benjamini_controlling_1995]. 
  - The problem is, we don't have a good expression for $E[n_{TP}]$. If we write $E[n_{TP}] = E[n_{reject}] * P(\neg true \vert reject)$, we don't have an expression for $P(\neg true \vert reject)$ like $FDR \leq \alpha$. If we just use the joint probability $E[n_{TP}] = n P(\neg true, reject)$, $P(\neg true, reject) =1 - \beta$, but there is no closed-form for power for B-H; [@benjamini_controlling_1995] had to run a simulation. So...
3. Screw it, just use the simulation results. Basically the original simulation.


```{r}
# (expected_payout <- expected_nselect %>%
#   mutate(E_tp = E_nselect * (1 - alpha),
#          E_tn = (nregions - E_nselect) * alpha,
#          E_fp = E_nselect * p_null,
#          E_fn = (nregions - E_nselect) * (1 - p_null),
#          E_payout = E_tp - 19 * E_fp + E_tn - E_fn)
# )

# (expected_payout <- expected_nselect %>%
#   mutate(E_tp = E_nselect * (1 - p_null), 
#          E_tn = (nregions - E_nselect) * p_null,
#          E_fp = E_nselect * p_null,
#          E_fn = (nregions - E_nselect) * (1 - p_null),
#          E_payout = E_tp - 19 * E_fp + E_tn - E_fn)
# )
df %>%
  group_by(method, nregions) %>%
  summarize(mean(pay))
```



## Grid search for the good alpha

Putting the above pieces together, we vary $alpha$ and $P(null)$ but keep $\mu$, $\sigma$ and $K$ the same. 

### Encapsulate simulation function

Each call to `finding_payout` has a different $\alpha$ value. Within each call, create `n_iter` sets of p-values

```{r}
finding_payout <- function(alpha_thres, p_null){
  df <-
    expand.grid(
      nregions = c(8, 12),
      iter = 1:n_iter
    ) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    group_by(iter, nregions) %>%
    mutate(
      mu = mu * rbinom(nregions, 1, 1 - p_null), 
      p_raw = map_dbl(mu, ~rp(.x, sigma)), 
      p_bh = p.adjust(p_raw, method = "BH"),
    )  %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp * fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - 19 * fp + tn - fn) * 1) 
  
df %>%
  group_by(method, nregions) %>%
  summarize(E_pay = mean(pay), 
            E_fdr = mean(fdr), .groups = "drop")
}

```

```{r}
expand.grid(
  alpha_thres = c(0.05, 0.1),
  p_null = c(0.4, 0.5, 0.6)
) %>%
  split(1:nrow(.)) %>%
  map_dfr(~cbind(.x, finding_payout(.$alpha_thres, .$p_null), row.names = NULL))
```

```{r}
sim_df <- seq(from = 0.05, to = 0.5, by = 0.01) %>%
  set_names() %>%
  map_df(~finding_payout(.), .id = "alpha") 
 


sim_df %>% 
  ggplot(aes(as.numeric(alpha), E_payout)) + 
  geom_point() + 
  facet_grid(. ~ nregions)


  
```



