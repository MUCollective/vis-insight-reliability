---
title: "Pilot 1"
output:
  html_document:
    df_print: paged
---
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstan)
library(brms)
library(dbplyr)
library(modelr)
library(tidybayes)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Summary

- Recruiting platform: Prolific
  - study code 29ABE0A0
  - base pay: $7
  - estimated time: 40 min
- Study website: https://mucollective.github.io/vis-insights-study/
- Database at AWS
- Exit survey at Qualtrics



## Participants

We load the data provided by Prolific and the logs that we collect in our database. We assign each participant a unique user ID. We then select participants which have completed the study (codes: APPROVED or AWAITING REVIEW)

```{r}
# df_prolific <- read.csv("pilot_1_prolific_first_two.csv")
df_prolific <- read.csv("pilot-1/pilot_1_prolific_alles.csv")
df_logs <- read.csv("pilot-1/userlog.csv")

completed_codes <- c("AWAITING REVIEW", "APPROVED")

#Those who completed
participants <- df_prolific %>%
  filter(status %in% completed_codes) %>%
  rename(prolific_pid = participant_id) %>% 
  inner_join(df_logs, by = "prolific_pid") %>%
  select(prolific_pid, modulo)
```

## Database response records and data cleaning

We store each participants' responses in a database. We import those responses and join them with the dataframe which contains the list of participants who have successfully completed the study. We also include the **correct** responses i.e. whether a region is actually profitable or not based on the total population, and not just the sample provided.

We wrangle the data into a format convenient for subsequent modeling and add columns which help us identify group-level effects. Since each participant in our pilot study participated in 90 trials, we index the trial number for each participant. The trials are also divided into blocks of three, which we index separately. Finally, we compute the payout for each trial.

```{r}
# The "solutions" to participant trials
df_soln <- read.csv("pilot-1/solutions.csv") %>% 
  group_by(trial) %>%
  summarise(
    positives = sum(profitable == TRUE),
    negatives = sum(profitable == FALSE)
  )

# Read data
df_db <- read.csv("pilot-1/pilot_1_db_all.csv")

# Select participants
df <- participants %>%
  inner_join(df_db, by="prolific_pid") %>%
  inner_join(df_soln, by = "trial") %>%
  arrange(id) %>%
  group_by(prolific_pid) %>%
  mutate(p_trial_id = row_number()) %>% # trial number 1...90
  group_by(prolific_pid, incentive) %>%
  mutate(
    alpha = (incentive/100),
    block_id = row_number(), # trial number within a bloc 1...30
    block = round(p_trial_id / 30) + 1,
    trial_pay = c(head(cumulative_pay, 1), diff(cumulative_pay)),
    fp = -1 * fp, # make it the number of false positives (count >= 0)
    fn = -1 * fn
  ) %>%
  ungroup() %>%
  select( -c(modulo, session_id, duration) )
```


### Evaluating problems in data collection

We check to see if there are any problems in data collection. Below, we check if the cumulative column is being calculated accurately, by comparing it to the calculation using the actual number of true positive, true negatives, false positives and false negatives that we record directly in the database. We find, in the first pilot, 3 instances where this is violated.

```{r}
df %>% 
  arrange(id) %>%
  group_by(prolific_pid) %>%
  mutate(calculated_trial_payout = (tp + tn - fn)*20 - fp*20*round((1 - alpha)/alpha)) %>%
  mutate(diffed_trial_payout = c(head(cumulative_pay, 1), diff(cumulative_pay))) %>%
  filter(diffed_trial_payout != calculated_trial_payout)
```

## Preliminary data exploration

Tally the false positives by alpha levels

```{r}
df %>%
  group_by(incentive, fp) %>%
  ggplot(aes(fp)) +
  geom_bar() + 
  facet_grid(. ~ incentive/100) +
  labs(x = "# of false positives")
```



## Exploratory models

### Beta regression model


Simulating the outcome of a completely random selection process

```{r}
df %>% 
  ungroup() %>%
  
```

We first calculated the variable `expected over optimal` which indicates in each trial, a participant's payoff as a fraction of the optimal payoff. We then predict this fraction using a beta regression model. We use `incentive` and `trial id` as a predictor as we vary these two variables in our study. We also model a unique intercept for each participant.

```{r}
df.betareg <- df %>% 
  ungroup() %>%
  mutate( 
    selected_positives = tp + abs(fp),
    payoff_trial = (tn + tp - fn) * 20  - fp * round((1 - alpha)/alpha) * 20,
    payoff_lower = - negatives * 20 * round((1 - alpha)/alpha) - positives * 20,
    payoff_upper = 20 * (positives + negatives),
    incentive = factor(incentive),
    expected_over_optimal = (payoff_trial - payoff_lower)/(payoff_upper - payoff_lower + 1)
  )

## need to identify and fix
## why certain participants 
## are scoring less than zero
```

```{r}
priors_beta = c(
  prior(normal(0, 1), class = b),
  prior(normal(2, 2), class = Intercept),
  prior(normal(2, 2), class = Intercept, dpar = phi),
  prior(normal(0, 1), class = b, dpar = phi),
  prior(student_t(3, 0, 1), class = sd)
)

# fit_beta <- brm(bf(expected_over_optimal ~ incentive*p_trial_id + (1|prolific_pid), 
#                   phi ~ incentive*p_trial_id), prior = priors_beta, 
#                   family = Beta, data = df.betareg, chains = 2, cores = 2, 
#                   control = list(adapt_delta = 0.99, max_treedepth = 15, stepsize = 0.005))

#fit_beta <- readRDS(file = "models/beta-reg.rds")
summary(fit_beta)

get_prior(bf(expected_over_optimal ~ incentive*p_trial_id + (1|prolific_pid), 
                   phi ~ incentive*p_trial_id), prior = priors_beta, 
                family = Beta, data = df.betareg)
```


```{r}
data_grid(data = df.model, p_trial_id = 1:90, incentive = as.factor(c(1, 5, 10))) %>%
  add_predicted_draws(fit_beta, re_formula = NA) %>%
  rename( trial = p_trial_id ) %>%
  ungroup() %>%
  ggplot() +
  stat_lineribbon(aes(x = trial, y = gtools::logit(.prediction), group = incentive), .width = c(.95, .8, .5), color = "#08519C") +
  facet_grid(. ~ incentive) +
  # ylim(c(0, 1)) +
  scale_fill_brewer()
```

### Zero inflated binomial model

In this model, we predict the probability of a person to indicate a region as positive in each trial. We use 

```{r}
priors_zib = c(
    prior(normal(0, 1), class = b),
    prior(normal(0, 1), class = Intercept),
    prior(normal(2, 2), dpar = zi),
    prior(exponential(2), class = sd)
)

# fit_zib <- brm(bf(selected_positives | trials(10) ~ incentive*block_id*block + (1|prolific_pid), 
#                 zi ~ incentive*block_id*block), prior = priors_zib, 
#                 family = zero_inflated_binomial, data = df.model, chains = 2, cores = 2)

# saveRDS(fit_zib, file = "models/fit-zib.rds")
# fit_zib <- readRDS(file = "models/fit-zib.rds")
summary(fit_zib)
```

  
```{r}
data_grid(data = df.model, block_id = 1:30, block = 1:3, incentive = as.factor(c(1, 5, 10))) %>%
  add_fitted_draws(fit_zib, dpar = c("mu", "zi"), n = 500, re_formula = NA) %>%
  ungroup() %>%
  mutate( trial_id = (block - 1) * 30 + block_id ) %>%
  ggplot() +
  geom_line(aes(x = trial_id, y = mu, colour = ordered(block), group = paste(block, .draw)), alpha = 1/10) +
  facet_grid(incentive ~ .) +
  scale_color_brewer(palette = "Dark2")
```

```{r}
data_grid(data = df.model, block_id = 1:30, block = 1:3, incentive = as.factor(c(1, 5, 10))) %>%
  add_fitted_draws(fit_zib, dpar = c("mu", "zi"), n = 500, re_formula = NA) %>%
  ungroup() %>%
  mutate( trial_id = (block - 1) * 30 + block_id ) %>%
  ggplot() +
  geom_line(aes(x = trial_id, y = zi, colour = ordered(block), group = paste(block, .draw)), alpha = 1/10) +
  scale_color_brewer(palette = "Dark2") +
  facet_grid(incentive ~ .)
```

### Multiple comparisons

```{r}
load("data/simulated_data.RData")

df.random_strategy_payout <- select(data.sample, -c(mu, sd, effect_size, mu_sd_pairs, data, store_idx, region, baseline, p_h1, idx) ) %>%
  mutate(p_value = round(map_dbl(profit, ~ t.test(.x)$p.value), 5)) %>%
  group_by(trial) %>% 
  ungroup() %>%
  mutate( 
    index = 1:nrow(.),
    .sim = map(index, ~ 1:1e3),
    random_selection = map(index, ~rbinom(1e3, 1, 0.5))
  ) %>%
  unnest(cols = c(random_selection, .sim)) %>%
  mutate( 
    payoff = ifelse(random_selection == 1 & is_profit == FALSE, -19, ifelse(random_selection == 0 & is_profit == TRUE, -1, 1))
  ) %>%
  group_by(trial, .sim) %>%
  summarise(payoff = sum(payoff)) %>%
  group_by(trial) %>%
  median_qi(payoff)

df.random_strategy_payout
```

```{r, warning = FALSE}
df.bh_correction <- select(data.sample, -c(mu, sd, effect_size, mu_sd_pairs, data, store_idx, region, baseline, p_h1, idx) ) %>%
  mutate(p_value = round(map_dbl(profit, ~ t.test(.x)$p.value), 5)) %>%
  group_by(trial) %>% 
  ungroup()

df.bh <- df %>% 
  select(-c(id, incentive, tp, tn, fp, fn, cumulative_pay, positives, negatives, block_id, block)) %>%
  left_join(df.bh_correction, by = "trial") %>%
  unnest(cols = c(p_value, region_idx, is_profit)) %>%
  group_by(prolific_pid, p_trial_id, region_idx) %>%
  mutate(
    responses = map(strsplit(as.character(responses), ","), as.numeric),
    selected =  any(region_idx %in% unlist(responses))
  ) %>%
  ungroup() %>%
  group_by(prolific_pid, p_trial_id) %>%
  arrange(prolific_pid, p_trial_id, p_value) %>%
  select(-responses) %>%
  mutate( 
    rank = 1:10,
    critical_value = rank/10 * alpha,
    bh_profit = ifelse(p_value < critical_value, TRUE, FALSE),
    bh_optimal_payout = ifelse(bh_profit == is_profit, 20, ifelse(bh_profit == TRUE & is_profit == FALSE, -20*(1-alpha)/alpha, -20))
  )
```

```{r}
select(data.sample, -c(mu, sd, effect_size, mu_sd_pairs, data, store_idx, region, baseline, p_h1, idx) ) %>%
  mutate(
    p_value = round(map_dbl(profit, ~ t.test(.x)$p.value), 5),
    alpha = 0.05 #map(trial, ~ c(0.1, 0.05, 0.01))
  ) %>%
  unnest(cols = c(alpha)) %>%
  group_by(trial, alpha) %>%
  arrange(alpha, trial, p_value) %>%
  mutate(
    rank = 1:10,
    critical_value = rank/10 * 0.05,
    bh_profit = ifelse(p_value < critical_value, TRUE, FALSE)
  ) %>%
  summarise( to_select = sum(bh_profit)) %>%
  ungroup() %>%
  mean_qi(to_select)
```

