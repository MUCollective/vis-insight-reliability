---
title: 'Stimuli data generation'
date: "10/28/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(modelr)
library(tidybayes)
library(furrr)
library(GGally)
```

# Introduction
The goal of this study is to investigate whether participants, when performing visual exploratory data analysis, perform some form of implicit multiple comparisons under a given incentive structure, and whether different visual representations may impact decision making. The multiple comparisons problem arises when participants are performing multiple hypothesis tests simultaneously, as the more number of inferences are attempted (i.e. comparisons performed), the more likely it is to get a False Positive Discovery. The relationship between the number of comparisons and the probability of making a False Discovery, conditional on the null hypothesis being true, is equal to $1 - (1 - \alpha)^n$, where $n$ is the number of comparisons performed.

In this document we perform a simulation to:
- compare the relationship between False Discovery Rate and $p_{null}$, probability of the null hypothesis being true
- compare False Discovery Rates under different statistical procedures which either do not correct for multiple comparisons or correct for family-wise error rate (Bonferroni), false discovery rate (Benjamini-Hochberg)
- compare the relationship between False Discovery Rate and $\alpha$, the significance level of the test
- compare payout under different incentive schemes for each correction procedure

# Study Design
Before we go further, it is important to list out aspects of our study design.

## Number of comparisons
The probability of a False Discovery is correlated with $n$. We test the following values of $n$: 1, 2, 4, 8, 12 and 16.

## Visualisation 
We are using the following visualisations as stimuli to study their impact on false discovery rates and decision making:

- scatterplot / bar chart / raw data
- mean point estimate with 95% confidence intervals
- probability density plots
- hypothetical outcome plots of raw data
- hypothetical outcome plots of quantiles of sampling distribution
- quantile dot plots

However, for the current simulation, these different chart types will not play a role

## P(null)
We manipulate the probability of the alternate hypothesis being true $P(H_1 = True) = 1 - p_{null}$, at consistent intervals between (0.5 and 1). We incentivise participants based on this probability.

- if $P(H_1 = True) = 0.5$, $Payoff_{TP} = Payoff_{FP} = Payoff_{TN} = Payoff_{FN}$
- if $P(H_1 = True) = 0.9$, $Payoff_{FP} = - 9 \times Payoff_{TP}$, $Payoff_{FN} = -1/9 \times Payoff_{TN}$ (_Note: we need to figure out the relationship between $Payoff_{TP}$ and $Payoff_{FN}$_)

For each value of $p_{null}$, we show participants k graphs with 10 data points in each graph. These graphs show the profits of 10 randomly sampled stores from a region with n stores. We ask them if, based on this sample, the stores are meeting a designated profit margin.

## Effect size
We follow Kale et al.'s study design and use an effect size of 0.9 (mean = 0.9, sd = 1)

# Simulating the data

Now that we have nailed down the variables in our experiment design we can begin to simulate the data. Before we begin, we set up the parameters which are relevant for our experiment design. We present participants with graphs of 20 points (sample size). To support multiple comparisons, we create graphs for at 6 different levels of "# of possible comparisons" as mentioned previously

```{r}
sample_size = 20 # num of points in each graph
n_trials = 100 # num of trials, here we use a large number for CLT
n_region = c(1, 2, 4, 8, 12, 16) # num of possible multiple comparisons
```

## The RNG function

First, we will simulate the data for the entire population. We use the function `RNG_data` for our simulation (more details below).

```{r}
RNG_data = function (n_regions, n, p_null, .seed = 1) {
  set.seed(.seed)
  
  replicate(n + 1, runif(n_trials * n_regions)) %>%
    as_tibble(.name_repair = ~ gsub("X.", "V", make.names(., unique = TRUE))) %>%
    rename_at(vars("V1":paste0("V", n)), ~ stringr::str_replace(., 'V', "store_")) %>%
    rename(mu = X) %>%
    mutate(mu = as.integer(mu > p_null), trial = rep(1:n_trials, n_regions), region = rep(1:n_regions, each = n_trials)) %>%
    select(trial, region, everything())
}
```

Let us break this down further. The function `runif` returns a uniform distribution of length = `n_trials * n_regions`. We use the replicate function to create a multivariate uniform distribution of dimension `sample_size + 1`. We then convert this into a tibble (using `as_tibble`) where each column represents one dimension of this multivariate uniform distribution. We then rename the columns to reflect the data that we are simulate. This is what the code looks like:

We consider the first column of this multivariate distribution (`mu`) to indicate whether a region is, on average, profitable or not. We consider the remaining rows to represent the quantiles of a normal distribution. The other parameters of the normal distribution (mean and variance) are determined based on the value of `mu`. We will expand on this later.

The final two lines of the function are to add two columns, indicating `trial` and `region`.

Thus, this is what the data looks like for 1 region (Note that we use a large value for # of trials to obtain an asymptotic estimate of False Discovery Rate under different conditions):

```{r}
RNG_data(n_regions = 1, sample_size, 1, 1)
```

Let's take a look at the pairs plot of the generated multivariate normal distribution.

```{r, fig.width = 16, fig.height = 9, eval = FALSE}
RNG_data(n_regions = 1, sample_size, 1, 1) %>%
  select(starts_with("store")) %>%
  ggpairs(lower = list(continuous = wrap("points", alpha = 0.5), combo = wrap("dot", alpha = 0.5)))
```

# Verifying the simulation

## The relationship between FDR and $\alpha$, if $p(null) = 1$
The first step in verifying if we are simulating the data correctly is by comparing the False Discovery Rates under different multiple comparison scenarios to the theoretical estimate. We use $p(null) = 1$ to generate the data:

First, we create a grid of the different multiple comparison scenarios while fixing the value of $p(null)$ at 1:

```{r}
(.dat_sim = crossing(n_regions = n_region, p_null = 1))
```
For each point in this grid, we use the `RNG_data` function to simulate values for each store:

```{r}
n_trials = 10000

.dat_sim = .dat_sim %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y))
  ) %>%
  unnest(values)

head(.dat_sim)
```

We then convert the data into the long format (we will subsequently summarise the data for each of the 20 stores into a list, which is not shown below):

```{r}
.dat_sim.long = .dat_sim %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu)

head(.dat_sim.long)
```
Finally, as mentioned in Section 3.1, the random numbers generated for each stores correspond to the quantiles of a normal distribution of mean 0.9 and standard deviation 1 (effect size = 0.9). Thus, after performing this transformation, for each trial and each region, we obtain 20 normally distributed data points, which represent the profits for the stores. These are contained in the `data` column of the following table:

```{r}
.dat_sim.norm = .dat_sim.long %>%
  summarise(data = list(profit), .groups = "drop") %>%
  mutate(data = map2(data, mu, ~ qnorm(.x, .y * 0.9)))

head(.dat_sim.norm)
```

The graph below shows what the simulated data looks like for 1 (out of the 200 simulated trials), when the number of possible comparisons is four:

```{r, fig.width = 12, fig.height = 3}
.dat_sim.norm %>%
  filter(n_regions == 4 & trial == 1) %>%
  unnest(data) %>%
  ggplot(aes(y = data, x = NA)) +
  geom_jitter( width = 0.05, alpha = 0.7 ) +
  facet_grid(. ~ region) +
  geom_hline( yintercept = 0, color = "red", alpha = 0.5 ) +
  labs(y = "Profit", x = "# of comparisons") +
  theme_minimal() +
  theme(
    axis.ticks.x = element_blank(), 
    axis.text.x = element_blank()
  )
```

We then put all of this together into a single code block, and perform a one-sided t-test for the alternative hypothesis $H_1$: *is the average profit for the set of 20 stores in the sample* $> 0$? ($H_0$: the average profit for the set of stores $\ngtr 0$):

```{r}
data.sample = crossing(n_regions = n_region, p_null = 1) %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y))
  ) %>%
  unnest(values) %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu) %>%
  summarise(data = list(profit), .groups = "drop") %>%
  mutate(data = map2(data, mu, ~ qnorm(.x, .y * 0.9))) %>%
  mutate(p = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value))
```

Based on the p-values, we then either reject or do not reject the null hypothesis at a signficance level of $\alpha = 0.5$. We then calculate the number of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN) for every trial. Next, for every trial, we calculate whether there were any FPs (since we want to verify whether the presence of the multiple comparisons problem in our simulated data, which claims that as the number of comparisons increases, the probability of a False Positive finding also increases). Next we take the average of the presence of False Positives across the set of 200 trials to obtain an asymptotic value for the probability of finding a False Positive.

We then compare the empirical values from our simulation to the theoretical estimate. For this simulation, we want to see that as the number of comparisons ($x$) increases, the probability of at least one False Positive finding also increases as: $f(x) = 1 - (0.95)^x$. This is the Family Wise Error rate that correction strategies such as Bonferroni attempts to control.

```{r}
alpha = 0.05
data.sample %>% 
  mutate(reject_null = p < alpha, `mu > 0` = mu != 0) %>% # reject or not?
  group_by(n_regions, p_null, trial) %>%
  mutate(TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
            FP = as.integer(!`mu > 0` & reject_null),
            TN = as.integer(!`mu > 0` & !reject_null),
            FN = as.integer(`mu > 0` & !reject_null),
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) %>% # calculates total number of TP, TN, FP, FN for each trial
  # because \pi_0 = 1, here, FDR will always be equal to 1
  # thus correcting for FDR is the same as correcting for FWER
  summarise(FWER = mean(FP > 0), .groups = "drop") %>% # estimates Pr(FP) across the set of trial
  mutate(`theoretical estimate` = 1 - 0.95^n_regions) %>%
  pivot_longer(cols = c(FWER, `theoretical estimate`)) %>%
  ggplot() +
  geom_point(aes(y = value, x = n_regions, color = name)) +
  geom_line(aes(y = value, x = n_regions, color = name))
```

We can see from our results (in the graph above) that this holds.

However, what we ideally want to control for is the average False Discovery Rate, which is what is achieved by Benjamini-Hochberg (or other similar multiple comparisons correction). However, we need different values of $\pi_0$ for that.

## Effect of p(null)

Next, we need to determine a value for the probability of the null hypothesis being true $P(null)$. The False Discovery Rate would vary based on $P(null)$, and we would like to see this relationship, as well as how $E(payout)$ is affected, before we select a particular value of $P(null)$. To check for this, we first generate the data we will use to create graphs, and compute the uncorrected and Benjamini-Hochberg p-values.

We first create the grid of `# comparisons` and `P(null)$ values:

```{r}
data.sample.grid = crossing(n_regions = n_region, p_null = seq(0.1, 0.9, by = 0.1))

head(data.sample.grid)
```

To this, we add draws from a multivariate uniform distribution which encodes the following two piece of information:
- whether a store is positive or not (computed as: x > p_null, where x is a random variable)
- quantile for the profit of a store
We use the previously defined function `RNG_data` for this

```{r}
data.sample.unif = data.sample.grid %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y, .seed = (.x * 10) + (.y * 10)))
  ) %>%
  unnest(values)

head(data.sample.unif)
```
Next, we process the data to be in the long format, which would make subsequent operations easier:

```{r}
data.sample_grouped.unif = data.sample.unif %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu) %>%
  summarise(data = list(profit), .groups = "drop")

head(data.sample_grouped.unif)
```

Recall that the column data currently stores the quantiles for the profit of 20 stores, based on a normal distribution. However, we want the actual profits. In the next step, we compute profits from the available quantile information using the `qnorm` function. We use mean of 0.9 and SD of 1. We store this simulated dataset to avoid recomputation.

```{r}
effect_size = 0.9

data.sample = data.sample_grouped.unif %>%
  nest(data = everything()) %>%
  expand(data, effect_size = effect_size) %>%
  unnest(data) %>%
  group_by(n_regions, p_null, trial) %>%
  mutate(
    mu = mu*effect_size,
    data = map2(data, mu, ~ qnorm(.x, mean = .y, sd = 1))
  )

saveRDS(data.sample, "sim_data_pr_sample_n10000.rds")
head(data.sample)
```

We then load the dataset, and perform a one-sided t-test for the alternative hypothesis $H_1$: *is the average profit for the set of 20 stores in the sample* $> 0$? ($H_0$: the average profit for the set of stores $\ngtr 0$). We then adjust the p-values using two multiple comparisons correction methods (Bonferroni and Benjamini-Hochberg):

```{r, eval = FALSE}
data.sample = readRDS("sim_data_pr_sample_n10000.rds")

plan(multisession, workers = 8)

data.sample.stats = data.sample %>%
  group_by(n_regions) %>%
  group_split() %>%
  future_map_dfr( 
    ~ mutate(., p_uncorrected = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)),
    .progress = TRUE
  ) %>%
  select(-data) %>%
  group_by(n_regions, p_null, trial) %>%
  mutate(
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni")
  ) %>%
  pivot_longer(cols = c("p_uncorrected", "p_BH", "p_bonferroni"), names_to = "method", values_to = "p.value", names_prefix = "p_")

plan(sequential)

saveRDS(data.sample.stats, "sim_data_pr_sample-stats-n10000.rds")
```


Next, we compute the TP/TN/FP/FN values, by controlling for alpha at the 0.05 level, which then allows up to estimate the False Discovery Rates. We visualise the False Discovery Rate as a function of `p(null)` for each type of correction and level of alpha. We plot it separately for the number of possible regions:

```{r, fig.width = 12, fig.height = 3}
data.sample.stats = readRDS("sim_data_pr_sample-stats-n10000.rds")

alpha = 0.05

data.sample.stats.summary = data.sample.stats %>%
  ungroup() %>%
  group_by(method, n_regions, p_null, trial) %>%
  mutate(
    reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

data.sample.stats.summary %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop") %>%
  ggplot() +
  geom_hline(aes(yintercept = alpha), color = "black") +
  geom_line(aes(x = p_null, y = FDR, color = method)) +
  facet_grid(. ~ n_regions, scales = "free_y") +
  theme_minimal()
```

```{r}
data.sample.stats.summary
```



In addition to the False Discovery rate, we need to evaluate how our proposed incentive structure changes for different values of `p(null)` (x-axis), for each level of the `# of possible comparisons`. We reward 1 point for each correct discovery (True Positive and True Negative), -1 for each False Negative, $-(1-\alpha)/alpha = -19$ for each False Positive:

```{r, fig.width = 12, fig.height = 3}
data.sample.stats.summary %>%
  mutate(payout = TP + TN - FN - (1-alpha)/alpha*FP) %>%
  summarise(E_payout = mean(payout), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = E_payout, color = method)) +
  facet_grid(. ~ n_regions, scales = "free_y") +
  theme_minimal()
```

From the graphs above, we notice that if $p(null) <= 0.5$, we might not observe an increased false discovery rate even for uncorrected methods, suggesting that we might want to use a higher value of $p(null)$. For the subsequent simulations, we use $p(null) = 0.8$.

## Impact of $\alpha$

However, different values of $\alpha$ might have an impact on the payout. Below, we investigate whether in our simulations, the correction strategies control for FDR at a particular $\lpah$ level. We then compare payout under different incentive schemes and values of $\alpha$.

We re-calculate the grid, with only one level of $p(null)$ but a greater number of trials to ensure asymptotic estimates. Like before, we save the data-structure to avoid recomputation. 

```{r}
n_region = c(1, 2, 4, 8, 12, 16, 20, 50, 100)

data.sample.unif.pnull_0.8 = crossing(n_regions = n_region, p_null = 0.8) %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y, .seed = (.x * 10) + (.y * 10)))
  ) %>%
  unnest(values) %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu) %>%
  summarise(data = list(profit), .groups = "drop")

effect_size = 0.9

data.sample.pnull_0.8 = data.sample.unif.pnull_0.8 %>%
  mutate(effect_size = effect_size) %>%
  group_by(n_regions, p_null, trial) %>%
  mutate(
    mu = mu*effect_size,
    data = map2(data, mu, ~ qnorm(.x, mean = .y, sd = 1))
  )

saveRDS(data.sample.pnull_0.8, "sim_data_pr_sample-pnull_0.8-n10000.rds")
```

We then perform the same sets of computations, where we compute the p-value and the adjusted p-value using the Benjamini-Hochberg and Bonferroni methods.

```{r}
data.sample.pnull_0.8 = readRDS("sim_data_pr_sample-pnull_0.8-n10000.rds")

data.sample.pnull_0.8.p_val = data.sample.pnull_0.8 %>%
  ungroup() %>%
  mutate(p = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value))

data.sample.pnull_0.8.stats = data.sample.pnull_0.8.p_val %>%
  select(-data) %>%
  rename(p_uncorrected = p) %>%
  group_by(n_regions, trial) %>%
  mutate(
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni")
  )

saveRDS(data.sample.pnull_0.8.stats, "sim_data_sample-stats-pnull_0.8-n10000.rds")
```

Next, we create an optimisation function which, for a given dataframe, alpha level, # of comparisons, and incentive structure, computes the optimal alpha level which maximises the payout

```{r}
alpha_optim = function(alpha, comparisons, method, TP_mult, FP_mult) {
  data.sample.pnull_0.8.p_val %>%
    filter(n_regions == comparisons) %>%
    mutate( p = p.adjust(p, method = method) ) %>%
    mutate( alpha = alpha ) %>%
    group_by(alpha, trial) %>%
    mutate(
      reject_null = p < alpha, `mu > 0` = mu != 0, # reject or not?
      TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
      FP = as.integer(!`mu > 0` & reject_null),
      TN = as.integer(!`mu > 0` & !reject_null),
      FN = as.integer(`mu > 0` & !reject_null)
    ) %>%
    summarise_at(.vars = vars(TP, FP, TN, FN), sum) %>%
    summarise(payout = mean(TP*TP_mult + TN - FN + FP*FP_mult)) %>%
    magrittr::extract2("payout")
}
```

Next, we expand our grid to include different values of $\alpha \in [0.001, 0.8]$. We then calculate the number of TP, TN, FP and FN for each trial, at at each level of $\alpha$

```{r, fig.width = 12, fig.height = 6}
data.sample.pnull_0.8.stats = readRDS("sim_data_sample-stats-pnull_0.8-n10000.rds")
  # pivot_longer(c("p_uncorrected", "p_BH", "p_bonferroni"), names_to = "method", names_prefix = "p_", values_to = "p")
  ## already in long format...

# what are the different FDRs that we want to control for?
alphas = c(
  seq(0.001, 0.01, by = 0.001),
  seq(0.01, 0.2, by = 0.01),
  seq(0.2, 0.8, by = 0.05)
)

plan(multisession, workers = 8)

data.sample.pnull_0.8.summary.alpha = data.sample.pnull_0.8.stats %>%
  ungroup() %>%
  group_by(trial) %>%
  group_split() %>%
  future_map_dfr(
    ~ crossing(., alpha = alphas) %>%
      group_by(method, n_regions, alpha, trial) %>%
      mutate(
        reject_null = p < alpha, `mu > 0` = mu != 0, # reject or not?
        TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
        FP = as.integer(!`mu > 0` & reject_null),
        TN = as.integer(!`mu > 0` & !reject_null),
        FN = as.integer(`mu > 0` & !reject_null)
      ) %>%
      summarise_at(.vars = vars(TP, FP, TN, FN), sum),
    .progress = TRUE
  )

plan(sequential)

saveRDS(data.sample.pnull_0.8.summary.alpha, file = "sim_data_pr_sample-summary-alpha-n10000.rds")
```

We employ different correction strategies (BH, Bonferroni, uncorrected) to control for the False Discovery Rate. But first, we would like to verify whether, in our simulation, the **correction strategies are able to control for FDR** at each level of $\alpha$. The red line below is $y = x$ line and indicates $\alpha = FDR$ i.e. it indicates if the False Discovery Rate is exactly equal to $alpha$. Any strategy which controls FDR at a particular $\alpha$ level will be close to or below this line.

```{r, fig.width = 12, fig.height = 3}
data.sample.pnull_0.8.summary.alpha = readRDS(file = "sim_data_pr_sample-summary-alpha-n10000.rds")

data.sample.pnull_0.8.summary = data.sample.pnull_0.8.summary.alpha %>%
  mutate(
    FDR = ifelse((TP + FP), FP/(FP + TP), 0),
    FNR = ifelse((TN + FN), FN/(FN + TN), 0)
  ) %>%
  summarise(E_FDR = mean(FDR), E_FNR = mean(FNR), E_TN = mean(TN), E_TP = mean(TP), E_FP = mean(FP), E_FN = mean(FN), .groups = "drop")

data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.2) %>%
  ggplot() +
  geom_line(aes(x = alpha, y = E_FDR, group = factor(n_regions), colour = n_regions)) +
  geom_line(data = tibble(x = seq(0.001, 0.2, by = 0.001), y = x), aes(x, y), color = "red") +
  facet_grid(. ~ method, scales = "free_y") +
  ylab("E(FDR)") +
  scale_x_log10() +
  theme_minimal()
```

In the figure above, we observe that the Benjamini-Hochberg method accounts for multiple comparisons correct and controls the False-Discovery Rate at any particular value of $\alpha$. The Bonferroni method is an overly conservative estimate as the number of possible multiple comparisons increases, which will result in a high number of False Negatives. Finally, the uncorrected strategy does not account for multiple comparisons, as indicated by the high number of *expected false discoveries*.

Now, assume that we are controlling for False Positives using a one-sided t-test at $\alpha = 0.05$. This implies $\Gamma$ = qnorm(0.95) = 1.645.

Let $P(null) = p_{0}$. Now, consider a situation with no multiple comparisons. We want a participant who is not aware of the underlying value of $P(null)$, is risk-neutral and is making an inference at a particular statistical significance level, $\alpha$. The probabilities are:

- false positive: $P(FP| p_{0}) = p_{0}\alpha$
- true positive: $P(TP | p_{0}) = 1 - p_{0}\alpha $
- false negative: $P(FN | p_{0}) = 1 - p_{0}(1 - \alpha)$ 
- true negative: $P(TN | p_{0}) = p_{0} (1 - \alpha)$

Next, we assume that the user has to make n comparisons. What is the expected false discovery rate i.e. the probability of at least one false positive:
$E(FDR| p_{0}) = (1 - (1 - p_{0}\alpha)^n)$

Similarly, the expected false negative rate:
$E(FNR| p_{0}) = (1 - (p_{0} (1 - \alpha))^n)$

However, a user does not know the value of $p_{0}$, so we would want to integrate over all values of $p_{0}$:

$$
\begin{align}
E(FDR) 
&= \int_{0}^{1} 1 - (1 - p_{0}\alpha)^n dp_{0} \\
&= \frac{(1 - p_0\alpha)^{n+1}}{(n + 1)}\alpha + p_{0} \Biggr]_{0}^{1} \\
&= 1 -  \frac{1 - (1 - \alpha)^{n+1}}{(n+1)\alpha} \\

E(FNR) 
&= \int_{0}^{1} 1 - ((1 - \alpha)p_{0})^n dp_{0} \\
&= p_{0} - \frac{(p_{0}(1 -\alpha))^n}{n + 1}  \Biggr]_{0}^{1} \\
&= 1 - \frac{(1 -\alpha)^n}{n + 1} \\
\end{align}
$$


This suggests that False Discovery and False Negative Rates would vary based on the number of possible comparisons. As such, our incentives should as well.

For n = 1, $E(FDR) = 0.025, E(FNR) = 0.025$
For n = 4, $E(FDR) = 0.09512375, E(FNR) = \frac{1 + \alpha}{2}$

```{r}
e_fdr = function(n, alpha = 0.05) {
  1 - (1 - (1 - alpha)^(n + 1))/(alpha*(n+1))
}

e_fnr = function(n, alpha = 0.05) {
  1 -  ((1 - alpha)^n)/(n+1)
}
```



```{r, fig.height = 8, fig.width = 12}
# payout scheme
df.payout = data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.1) %>%
  mutate(payout = (((1 - alpha)/alpha) * E_TP) + E_TN - (((1 - alpha)/alpha) * E_FP) - E_FN)

df.payout %>%
  ggplot() +
  geom_line(aes(x = alpha, y = log(payout), group = method, colour = method)) +
  facet_wrap( ~ n_regions, scales = "free_y") +
  theme_minimal()
```





is making inference completely at random, the following are the probabilities of making an inference which is a:
- true negative: $\int_{0}^{1} p_{0} dp_{0} = p_{0}^2/2$
- false negative: $\int_{0}^{1} (1 - p_{0}) dp_{0} = p_{0} - p_{0}^2/2$ 
- true positive: $\int_{0}^{1} (1 - p_{0}) dp_{0} = p_{0} - p_{0}^2/2$
- false positive: $\int_{0}^{1} p_{0} dp_{0} = p_{0}^2/2$

Let's illustrate this with a simulation of a risk-neutral individual who is unaware of the underlying value of $P(null)$. Because such an individual has no information, they should be guessing completely at random (i.e. $p = 0.5$). We achieve such a behavior with the following incentive scheme: TP: 1, TN: 1, FP: -1, FN: -1.

```{r, simulation-1}
sim1 = function(p = 0.8, n = 1e2) {
  truth = rbinom(n, 1, prob = p)
  decision = rbinom(n, 1, prob = 0.5)
  TP = as.integer((truth == 1) & (decision == 1))
  TN = as.integer((truth == 0) & (decision == 0))
  FP = as.integer((truth == 0) & (decision == 1))
  FN = as.integer((truth == 1) & (decision == 0))
  sum(TP  + TN - FN - FP)
}

data.frame(x = 1:1e5) %>%
  mutate(payout = map_dbl(x, ~ sim1())) %>%
  ggplot() +
  geom_histogram(aes(payout))
```

However, this relation tells us the relationship between a true and a false negative, and that between a true and a false positive, but it is unclear what the relationship is between a true positive and a true negative.

In our sample, since $P(null) = 0.8$, a true *negative* is four times more likely than a false *negative* while a false *positive* is four times more likely than a true *positive*



Thus, for $p_{0} = 0.8$ and $\alpha = 0.05$, the probabilities would be:
- true negative: 0.76
- false negative: 0.19
- true positive: 0.01
- false positive: 0.04

From this, we know that a true negative is *76 times* more likely than a true positive. Given this, can we determine an incentive structure?

|                |     Ground Truth    |
| User Decision  | Positive | Negative |
| Positive       |   76     |    -19   |
| Negative       |   -4     |      1   |


```{r, fig.height = 8, fig.width = 12}
# payout scheme
df.payout = data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.1) %>%
  mutate(payout = (((1 - alpha)/alpha) * 4 * E_TP) + E_TN - (((1 - alpha)/alpha) * E_FP) - (4 * E_FN))

df.payout %>%
  ggplot() +
  geom_line(aes(x = alpha, y = log(payout), group = method, colour = method)) +
  facet_wrap( ~ n_regions, scales = "free_y") +
  theme_minimal()
```

$f(x) = 1 - (0.95)^x$


```{r, eval = FALSE}
data.sample.stats = readRDS("sim_data_pr_sample-stats-n10000.rds")

alphas = c(
  seq(0.001, 0.01, by = 0.001),
  seq(0.01, 0.2, by = 0.01)
)

plan(multisession, workers = 8)

data.sample.summary.alpha = data.sample.stats %>%
  ungroup() %>%
  group_by(trial) %>%
  group_split() %>%
  future_map_dfr(
    ~ crossing(., alpha = alphas) %>%
      group_by(method, p_null, n_regions, alpha, trial) %>%
      mutate(
        reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
        TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
        FP = as.integer(!`mu > 0` & reject_null),
        TN = as.integer(!`mu > 0` & !reject_null),
        FN = as.integer(`mu > 0` & !reject_null)
      ) %>%
      summarise_at(.vars = vars(TP, FP, TN, FN), sum),
    .progress = TRUE
  )

plan(sequential)

saveRDS(data.sample.summary.alpha, file = "sim_data-all_pr-summary-alpha-n10000.rds")
```

How about some simulations?

```{r}
data.sample.summary.alpha = readRDS(file = "sim_data-all_pr-summary-alpha-n10000.rds")
data.sample.summary.alpha
```



More simulations:



```{r, fig.width = 12, fig.height = 3}
n_1 = 2 # how much are we weighting FDRs 
n_2 = 1 # how much are we weighting FNRs 

# payout scheme
df.payout = data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.1) %>% 
  mutate(payout = (n_1 * (1 - E_FDR) + n_2 * (1 - E_FNR)) * 10) # 10 is just multiplier of the entire payout

d.BH  = df.payout %>%
  filter(method == "BH")

d.null  = df.payout %>%
  filter(method == "uncorrected") 

map_dfr(unique(d.BH$n_regions), ~ merge(filter(d.BH, n_regions == .), filter(d.null, n_regions == .), by = NULL)) %>%
  filter(round(alpha.x, 4) %in% c(0.005, 0.01, 0.05, 0.10)) %>%
  mutate(diff = payout.x - payout.y) %>%
  # filter(n_regions.x == 2) %>%
  ggplot() +
  geom_line(aes(x = alpha.y, y = diff, group = factor(n_regions.x), colour = (diff >= 0))) +
  facet_grid(. ~ alpha.x, scales = "free_y") +
  scale_x_log10() +
  # coord_cartesian(ylim = c(0, 10)) +
  theme_minimal()
```

This is what the simulated payout looks like under different alphas using the incentive scheme from our previous study: 

```{r, fig.width = 12, fig.height = 3}
# original payout scheme
df.payout = data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.1) %>% 
  mutate(payout = E_TP + E_TN - 9*E_FP - E_FN)

d.BH  = df.payout %>%
  filter(method == "BH")

d.null  = df.payout %>%
  filter(method == "uncorrected") 

map_dfr(unique(d.BH$n_regions), ~ merge(filter(d.BH, n_regions == .), filter(d.null, n_regions == .), by = NULL)) %>%
  filter(round(alpha.x, 4) %in% c(0.005, 0.01, 0.05, 0.10)) %>%
  mutate(diff = payout.x - payout.y) %>%
  ggplot() +
  geom_line(aes(x = alpha.y, y = diff, group = factor(n_regions.x), colour = (diff >= 0))) +
  facet_grid(. ~ alpha.x, scales = "free_y") +
  scale_x_log10() +
  theme_minimal()
```


```{r}
data.sample.pnull_0.8.summary %>%
  select(-E_FNR) %>%
  filter(alpha <= 0.2) %>%
  pivot_wider(names_from = method, values_from = E_FDR) %>%
  mutate(E_FDR_diff = BH - uncorrected) %>%
  ggplot() +
  geom_line(aes(x = alpha, y = -E_FDR_diff, group = factor(n_regions), colour = n_regions)) +
  # geom_line(data = tibble(x = seq(0.001, 0.2, by = 0.001), y = x), aes(x, y), color = "red") +
  # facet_grid(. ~ method, scales = "free_y") +
  scale_x_log10() +
  theme_minimal()
```


What is the evidence that we need to 
- given a particular alpha value, people should be incentivised to perform multiple comparisons correction if the number of regions changes. If people use an uncorrected strategy they will perform worse than a corrected strategy.
- Payout(x, correction_strategy, n_regions): Payout(x | correction_strategy \in {bonferroni, BH}) > Payout(x | correction_strategy == uncorrected)
- $argmax_\alpha E(payout | n regions, strategy = BH)$ is constant regardless of n regions
- $argmax_\alpha E(payout | n regions, strategy = uncorrected)$ is not constant
- $\exists \alpha_{BH} s.t. E(payout | nregions, strategy = BH, \alpha_{BH}) > E(payout | nregions, strategy = uncorrected, \alpha = \alpha_i) \forall \alpha_i \in (0,1)$
- $\exists \alpha_{BH} \wedge \exists nregions s.t. E(payout | nregions, strategy = BH, \alpha_{BH}) >  E(payout | nregions, strategy = uncorrected, \alpha = \alpha_i) \forall \alpha_i \in (0,1)$

if we pick a fixed alpha for every value of n_regions for an uncorrected strategy, eventually we will have an n_regions where BH strategy will dominate uncorrected

However, this does not take into account any potential False Negatives. We want to present participants with a scenario which resembles real world decision making---in the real world, there are obvious penalties for False Negatives. Below, we attempt to define an incentive scheme which takes into account both the False Discovery Rate and the False Negative Rate:


```{r, fig.width = 12, fig.height = 3}
data.sample.pnull_0.8.summary %>%
  filter(alpha <= 0.2) %>%
  ggplot() +
  geom_line(aes(x = alpha, y = E_FNR, group = factor(n_regions), colour = n_regions)) +
  # geom_line(data = tibble(x = seq(0.001, 0.2, by = 0.001), y = x), aes(x, y), color = "red") +
  facet_grid(. ~ method, scales = "free_y") +
  scale_x_log10() +
  theme_minimal()
```


```{r, fig.width = 12, fig.height = 3}
data.sample.pnull_0.8.summary %>%
  mutate(E_payout = (1 - E_FDR) + (1 - E_FNR)) %>%
  filter(alpha <= 0.2) %>%
  ggplot() +
  geom_line(aes(x = alpha, y = E_payout, group = factor(n_regions), colour = n_regions)) +
  geom_vline(xintercept = 0.05, color = "red") +
  facet_grid(. ~ method, scales = "free_y") +
  scale_x_log10() +
  theme_minimal()
```



## Comparisons with theoretical estimates

So far, we have performed simulations to understand how different statistical correction methods, significance levels for statistical tests and properties of the data generating process (such as $p(null)$) affect False Discovery Rates and Expected Payout under an incentive scheme.

### p-value PDF, CDF, invCDF definitionss

- Definitions of PDF and CDF of the $p$-value from [@hung_behavior_1997]
- Random draws from the $p$-value PDF `rp` is sampled through random draws from the uniform [0, 1] quantile space, then looked up through the inverse CDF function (using `uniroot`). 
- **Limitation**: These functions are dependent on the $\mu$, $\sigma$, $K$ parameters. These parameters are most likely different IRL but we are not using other values yet.

```{r}
K = sample_size
# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}
# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu/sigma)
}
# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}
# random sample from PDF of p-value using its invCDF
rp <- function(mu, sigma){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```

Next, we create a function which calculates the average false discovery rate and payout (across the set of trials):

```{r}
fp_penalty = 19
n_trials = 10000

calculate_payout = function(alpha, p_null) {
  expand_grid(nregions = n_region, iter = 1:n_trials, mu = 0.9, sigma = 1) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    mutate(unif = map_dbl(row_number(), ~ sobol(1, seed = ., scrambling = 2))) %>%
    group_by(nregions, iter) %>%
    mutate(
      mu = ifelse(unif > p_null, mu, 0),
      p_raw = map2_dbl(mu, sigma, ~rp(.x, .y)), 
      p_bh = p.adjust(p_raw, method = "BH"),
      p_bonf = p.adjust(p_raw, method = "bonferroni"),
    ) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp + fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - fp_penalty * fp + tn - fn) * 1) %>%
    group_by(method, nregions) %>%
    summarize(E_pay = mean(pay), E_fdr = mean(fdr), .groups = "drop")
}
```

We then create a grid of different $\alpha$ and $p(null)$ values which are passed as arguments to the `calculate_payout` function defined above. As this takes a long time to execute, we store it in a .rds file which we then load subsequently:

```{r}
plan(multisession, workers = 8)

data.sim.alpha_pnull = expand_grid(
    alpha = seq(0.01, 0.15, by = 0.01),
    p_null = 0.8 #seq(0.04, 1, by = 0.02)
  ) %>%
  mutate(sim = future_map2(alpha, p_null, calculate_payout, .options = furrr_options(seed = TRUE), .progress = TRUE)) %>%
  unnest(sim)

plan(sequential)

data.sim.alpha_pnull %>%
  saveRDS(file = "simulation-invCDF-alpha_pnull.rds")
```

The following plot shows the simulated results of payout as a function of alpha and p(null)

```{r, fig.width = 12, fig.height = 3}
data.sim.alpha_pnull = readRDS(file = "simulation-invCDF-alpha_pnull.rds")

data.sim.alpha_pnull %>%
  filter(p_null == 0.8) %>%
  group_by(alpha, method, nregions) %>%
  summarise(value = mean(E_pay), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = alpha, y = value, colour = nregions, group = nregions)) +
  facet_grid(. ~ method) +
  scale_x_log10() +
  theme_minimal()
```

```{r, fig.width = 12, fig.height = 3}
data.sim.alpha_pnull %>%
  filter(alpha == 0.05) %>%
  ggplot() +
  geom_line(aes(x = p_null, y = E_pay, colour = method)) +
  facet_grid(. ~ nregions, scales = "free_y") +
  theme_minimal()
```


## Addiitonal checks

```{r FDR and payout}
df_fdr_payout =  data.sample.stats.summary %>%
  group_by(n_regions, p_null, method, trial) %>% 
  summarize(across(TP:FN, ~ sum(.)), .groups = "drop") %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0), 
         payout = TP + TN - FN - (19*FP))  
```

Using Boole's inequality and the fact that p-value is uniformly distributed when $H_0$ is true.

${\displaystyle {\text{FWER}}=P\left\{\bigcup _{i=1}^{m_{0}}\left(p_{i}\leq {\frac {\alpha }{m}}\right)\right\}\leq \sum _{i=1}^{m_{0}}\left\{P\left(p_{i}\leq {\frac {\alpha }{m}}\right)\right\}=m_{0}{\frac {\alpha }{m}}\leq \alpha .}$


Is OK if FWER is > 0.05 from the simulation because p-value is uniformly distributed?

```{r checking bonferroni}
# overall FWER
df_fdr_payout %>%
  filter(method == "bonferroni") %>%
  group_by(n_regions, p_null, trial) %>% 
  summarize(V = any(FP), .groups = "drop")  %>%
  summarize(FWER = mean(V), .groups = "drop")
# by n_regions, p_null
df_fdr_payout %>%
  filter(method == "bonferroni") %>%
  group_by(n_regions, p_null, trial) %>% 
  summarize(V = any(FP), .groups = "drop_last") %>%
  summarize(FWER = mean(V), .groups = "drop") %>%
  filter(FWER > 0.05)
```

Check if p-values are uniform under $H_0$ ... sure

```{r}
data.sample.stats %>%
  filter(method == "uncorrected", mu == 0) %>%
  ggplot(aes(x = p)) + 
  geom_histogram(binwidth = 0.01)
```