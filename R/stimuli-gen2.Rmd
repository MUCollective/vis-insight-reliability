---
title: 'Stimuli data generation'
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(modelr)
library(tidybayes)
library(randtoolbox)
library(furrr)
```


# Introduction
The goal of this study is to investigate whether participants, when performing visual exploratory data analysis, perform some form of implicit multiple comparisons under a given incentive structure, and whether different visual representations may impact decision making. The multiple comparisons problem arises when participants are performing multiple hypothesis tests simultaneously, as the more number of inferences are attempted (i.e. comparisons performed), the more likely it is to get a False Positive Discovery. The relationship between the number of comparisons and the probability of making a False Discovery, conditional on the null hypothesis being true, is equal to $1 - (1 - \alpha)^n$, where $n$ is the number of comparisons performed.

In this document we perform a simulation to:
- compare the relationship between False Discovery Rate and $p_{null}$, probability of the null hypothesis being true
- compare False Discovery Rates under different statistical procedures which either do not correct for multiple comparisons or correct for family-wise error rate (Bonferroni), false discovery rate (Benjamini-Hochberg)
- compare the relationship between False Discovery Rate and $\alpha$, the significance level of the test
- compare payout under different incentive schemes for each correction procedure

# Study Design
Before we go further, it is important to list out aspects of our study design.

## Number of comparisons
The probability of a False Discovery is correlated with $n$. We test the following values of $n$: 1, 2, 4, 8, 12 and 16.

## Visualisation 
We are using the following visualisations as stimuli to study their impact on false discovery rates and decision making:

- scatterplot / bar chart / raw data
- mean point estimate with 95% confidence intervals
- probability density plots
- hypothetical outcome plots of raw data
- hypothetical outcome plots of quantiles of sampling distribution
- quantile dot plots

However, for the current simulation, these different chart types will not play a role

## P(null)
We manipulate the probability of the alternate hypothesis being true $P(H_1 = True) = 1 - p_{null}$, at consistent intervals between (0.5 and 1). We incentivise participants based on this probability.

- if $P(H_1 = True) = 0.5$, $Payoff_{TP} = Payoff_{FP} = Payoff_{TN} = Payoff_{FN}$
- if $P(H_1 = True) = 0.9$, $Payoff_{FP} = - 9 \times Payoff_{TP}$, $Payoff_{FN} = -1/9 \times Payoff_{TN}$ (_Note: we need to figure out the relationship between $Payoff_{TP}$ and $Payoff_{FN}$_)

For each value of $p_{null}$, we show participants k graphs with 10 data points in each graph. These graphs show the profits of 10 randomly sampled stores from a region with n stores. We ask them if, based on this sample, the stores are meeting a designated profit margin.

## Effect size
We follow Kale et al.'s study design and use an effect size of 0.9 (mean = 0.9, sd = 1)

# Simulating the data

Now that we have nailed down the variables in our experiment design we can begin to siimulate the data. Before we begin, we set up the parameters which are relevant for our experiment design. We present participants with graphs of 20 points (sample size). To support multiple comparisons, we create graphs for at 6 different levels of "# of possible comparisons" as mentioned previously

```{r}
sample_size = 20 # num of points in each graph
n_trials = 200 # num of trials, here we use a large number for CLT
n_region = c(1, 2, 4, 8, 12, 16) # num of possible multiple comparisons
```

## The RNG function

First, we will simulate the data for the entire population. We use the function `RNG_data` for our simulation (more details below).


```{r}
RNG_data = function (n_regions, n, p_null, .seed = 1) {
  sobol(n_trials * n_regions, dim = n + 1, scrambling = 3, seed = .seed) %>% # gives a multivariate uniform distribution
    as_tibble(.name_repair = ~ gsub("X.", "V", make.names(., unique = TRUE))) %>%
    rename_at(vars("V1":paste0("V", n)), ~ stringr::str_replace(., 'V', "store_")) %>%
    rename(mu = X) %>%
    mutate(mu = as.integer(mu > p_null), trial = rep(1:n_trials, n_regions), region = rep(1:n_regions, each = n_trials)) %>%
    select(trial, region, everything())
}
```

Let us break this down further. The function `sobol` returns a multivariate uniform distribution (where n = `sample_size + 1`). This multivariate distribution is a $m \times n$ vector (where $m = \textrm{# of trials}$). We then convert this into a tibble (using `as_tibble`) where each column represents one dimension of this multivariate uniform distribution. We then rename the columns to reflect the data that we are simulate. This is what the code looks like:

```{r}
sobol(n_trials * 1, dim = sample_size + 1, scrambling = 3, seed = 1) %>% 
    as_tibble(.name_repair = ~ gsub("X.", "V", make.names(., unique = TRUE))) %>%
    rename_at(vars("V1":paste0("V", sample_size)), ~ stringr::str_replace(., 'V', "store_")) %>%
    rename(mu = X)
```

We consider the first column of this multivariate distribution (`mu`) to indicate whether a region is, on average, profitable or not. We consider the remaining rows to represent the quantiles of a normal distribution. The other parameters of the normal distribution (mean and variance) are determined based on the value of `mu`. We will expand on this later.

The final two lines of the function are to add two columns, indicating `trial` and `region`.

Thus, this is what the data looks like for 1 region (Note that we use a large value for # of trials to obtain an asymptotic estimate of False Discovery Rate under different conditions):

```{r}
RNG_data(n_regions = 1, sample_size, 1, 1)
```


## The relationship between FDR and $\alpha$, if $p(null) = 1$
First, we want to verify if we are simulating the data correctly, by comparing the False Discovery Rates under different multiple comparison scenarios to the theoretical estimate. We use $p(null) = 1$ to generate the data:

First, we create a grid of the different multiple comparison scenarios and valuess of $p(null)$:

```{r}
(.dat_sim = crossing(n_regions = n_region, p_null = 1))
```
For each point in this grid, we use the `RNG_data` function to simulate values for each store:

```{r}
(.dat_sim = .dat_sim %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y))
  ) %>%
  unnest(values))
```
We then convert the data into the long format (we will subsequently summarise the data for each of the 20 stores into a list, which is not shown below):

```{r}
(.dat_sim = .dat_sim %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu))
```
Finally, as mentioned in Section 3.1, the random numbers generated for each stores correspond to the quantiles of a normal distribution of mean 0.9 and standard deviation 1 (effect size = 0.9). Thus, after performing this transformation, for each trial and each region, we obtain 20 normally distributed data points, which represent the profits for the stores. These are contained in the `data` column of the following table:

```{r}
(.dat_sim = .dat_sim %>%
  summarise(data = list(profit), .groups = "drop") %>%
  mutate(data = map2(data, mu, ~ qnorm(.x, .y * 0.9))))
```
The graph below shows what the simulated data looks like for 1 (out of the 200 simulated trials), when the number of possible comparisons is four:

```{r, fig.width = 12, fig.height = 3}
.dat_sim %>%
  filter(n_regions == 4 & trial == 1) %>%
  unnest(data) %>%
  ggplot(aes(y = data, x = NA)) +
  geom_jitter( width = 0.05, alpha = 0.7 ) +
  facet_grid(. ~ region) +
  geom_hline( yintercept = 0, color = "red", alpha = 0.5 ) +
  labs(y = "Profit", x = "# of comparisons") +
  theme_minimal() +
  theme(
    axis.ticks.x = element_blank(), 
    axis.text.x = element_blank()
  )
```

We then put all of this together into a single code block, and perform a one-sided t-test for the alternative hypothesis $H_1$: *is the average profit for the set of 20 stores in the sample* $> 0$? ($H_0$: the average profit for the set of stores $\ngtr 0$):

```{r}
data.sample = crossing(n_regions = n_region, p_null = 1) %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y))
  ) %>%
  unnest(values) %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu) %>%
  summarise(data = list(profit), .groups = "drop") %>%
  mutate(data = map2(data, mu, ~ qnorm(.x, .y * 0.9))) %>%
  mutate(p = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value))
```

Based on the p-values, we then either reject or do not reject the null hypothesis at a signficance level of $\alpha = 0.5$. We then calculate the number of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN) for every trial. Next, for every trial, we calculate whether there were any FPs (since we want to verify whether the presence of the multiple comparisons problem in our simulated data, which claims that as the number of comparisons increases, the probability of a False Positive finding also increases). Next we take the average of the presence of False Positives across the set of 200 trials to obtain an asymptotic value for the probability of finding a False Positive.

We then compare the empirical values from our simulation to the theoretical estimate. For this simulation, we can see that as the number of comparisons ($x$) increases, the probability of a False Positive finding also increases as: $f(x) = 1 - (0.95)^x$. We can see from our results that this holds:

```{r}
alpha = 0.05

data.sample %>% 
  mutate(reject_null = p < alpha, `mu > 0` = mu != 0) %>% # reject or not?
  group_by(n_regions, p_null, trial) %>%
  mutate(TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
            FP = as.integer(!`mu > 0` & reject_null),
            TN = as.integer(!`mu > 0` & !reject_null),
            FN = as.integer(`mu > 0` & !reject_null),
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) %>% # calculates total number of TP, TN, FP, FN for each trial
  mutate(FDR = ifelse(FP+TP == 0, 0, FP/(FP+TP))) %>% # calculates the average FDR for each trial
  summarise(FDR = mean(FDR), .groups = "drop") %>% # estimates Pr(FP) across the set of trial
  mutate(`theoretical estimate` = 1 - 0.95^n_regions) %>%
  pivot_longer(cols = c(FDR, `theoretical estimate`)) %>%
  ggplot() +
  geom_point(aes(y = value, x = n_regions, color = name)) +
  geom_line(aes(y = value, x = n_regions, color = name))
```

## Effect of p(null)

Next, we need to determine a value for the probability of the null hypothesis being true $P(null)$. The False Discovery Rate would vary based on $P(null)$, and we would like to see this relationship, as well as how  $E(payout)$ is affected. To check for this, we first generate the data we will use to create graphs, and compute the uncorrected and Benjamini-Hochberg p-values.

We first create the grid of `# comparisons` and `P(null)$ values:

```{r}
(data.sample.grid = crossing(n_regions = n_region, p_null = seq(0.1, 0.9, by = 0.1)))
```

To this, we add draws from a multivariate uniform distribution which encodes the following two piece of information:
- whether a store is positive or not (computed as: x > p_null, where x is a random variable)
- quantile for the profit of a store
We use the previously defined function `RNG_data` for this

```{r}
data.sample.unif = data.sample.grid %>%
  mutate(
    values = map2(n_regions, p_null, ~ RNG_data(.x, n = sample_size, .y, .seed = (.x * 10) + (.y * 10)))
  ) %>%
  unnest(values)

head(data.sample.unif)
```
Next, we process the data to be in the long format, which would make subsequent operations easier:

```{r}
data.sample_grouped.unif = data.sample.unif %>%
  pivot_longer(cols = starts_with("store_"), names_to = "store", names_prefix = "store_", values_to = "profit") %>%
  group_by(n_regions, p_null, trial, region, mu) %>%
  summarise(data = list(profit), .groups = "drop")

head(data.sample_grouped.unif)
```

Recall that the column data currently stores the quantiles for the profit of 20 stores, based on a normal distribution. However, we want the actual profits. In the next step, we compute profits from the available quantile information using the `qnorm` function. We use mean of 0.9 and SD of 1. We store this simulated dataset to avoid recomputation.

```{r}
effect_size = 0.9

data.sample = data.sample_grouped.unif %>%
  nest(data = everything()) %>%
  expand(data, effect_size = effect_size) %>%
  unnest(data) %>%
  group_by(n_regions, p_null, trial) %>%
  mutate(
    mu = mu*effect_size,
    data = map2(data, mu, ~ qnorm(.x, mean = .y, sd = 1))
  )

saveRDS(data.sample, "sim_data_sample_n200.rds")

head(data.sample)
```
We then load the dataset, and perform a one-sided t-test for the alternative hypothesis $H_1$: *is the average profit for the set of 20 stores in the sample* $> 0$? ($H_0$: the average profit for the set of stores $\ngtr 0$). We then adjust the p-values using two multiple comparisons correction methods (Bonferroni and Benjamini-Hochberg):

```{r}
data.sample = readRDS("sim_data_sample_n200.rds")

data.sample.stats = data.sample %>%
  mutate(
    p_uncorrected = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value), # calculates p-value
    p_BH = p.adjust(p_uncorrected, "BH"),
    p_bonferroni = p.adjust(p_uncorrected, "bonferroni")
  ) %>%
  select(-data) %>%
  pivot_longer(c("p_uncorrected", "p_BH", "p_bonferroni"), names_to = "method", names_prefix = "p_", values_to = "p")
```

Next, we compute the TP/TN/FP/FN values, by controlling for alpha at the 0.05 level, which then allows up to estimate the False Discovery Rates. We visualise the False Discovery Rate as a function of `p(null)` for each type of correction and level of alpha. We plot it separately for the number of possible regions:

```{r, fig.width = 12, fig.height = 3}
alpha = 0.05

data.sample.stats.summary = data.sample.stats %>%
  ungroup() %>%
  # expand(nesting(region, mu, p, method, n_regions, p_null, trial), alpha = alphas) %>%
  group_by(method, n_regions, p_null, effect_size, trial) %>%
  mutate(
    reject_null = p < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

data.sample.stats.summary %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop") %>%
  ggplot() +
  geom_hline(aes(yintercept = alpha), color = "black") +
  geom_line(aes(x = p_null, y = FDR, color = method)) +
  facet_grid(. ~ n_regions, scales = "free_y") +
  theme_minimal()
```

In addition to the False Discovery rate, we need to evaluate how our proposed incentive structure changes for different values of `p(null)` (x-axis), for each level of the `# of possible comparisons`. We reward 1 point for each correct discovery (True Positive and True Negative), -1 for each False Negative, $-(1-\alpha)/alpha = -19$ for each False Positive:

```{r, fig.width = 12, fig.height = 3}
data.sample.stats.summary %>%
  mutate(payout = TP + TN - FN - (1-alpha)/alpha*FP) %>%
  summarise(E_payout = mean(payout), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = E_payout, color = method)) +
  facet_grid(. ~ n_regions, scales = "free_y") +
  theme_minimal()
```

From the graphs above, we notice that if $p(null) <= 0.5$, we might not observe an increased false discovery rate even for uncorrected methods, suggesting that we might want to use a higher value of $p(null)$. For the subsequent simulations, we use $p(null) = 0.8$.

## Impact of $\alpha$

However, different values of $\alpha$ might have an impact on the payout. Below, we calculate, for different incentive schemes, the value of $\alpha$ that maximises payout:

```{r}
data.sample.p_val = data.sample %>%
  filter(p_null == 0.8) %>%
  mutate(
    p = map_dbl(data, ~ t.test(.x, alternative = "greater")$p.value)
  ) %>%
  select(-data)

alpha_optim = function(alpha, comparisons, method, TP_mult, FP_mult) {
  data.sample.p_val %>%
    filter(n_regions == comparisons) %>%
    mutate( p = p.adjust(p, method = method) ) %>%
    expand( nesting(region, mu, p), alpha = alpha ) %>%
    group_by(alpha, trial) %>%
    mutate(
      reject_null = p < alpha, `mu > 0` = mu != 0, # reject or not?
      TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
      FP = as.integer(!`mu > 0` & reject_null),
      TN = as.integer(!`mu > 0` & !reject_null),
      FN = as.integer(`mu > 0` & !reject_null)
    ) %>%
    summarise_at(.vars = vars(TP, FP, TN, FN), sum) %>%
    summarise(payout = mean(TP*TP_mult + TN - FN + FP*FP_mult)) %>%
    magrittr::extract2("payout")
}

plan(multisession, workers = 8)

df.alpha_optimal = crossing(
  comparisons = n_region,
  method = c("BH", "bonferroni", "none"),
  nesting(TP_mult = c(1, 1, 1, 9), FP_mult = c(-1, -19, -9, -1))
) %>%
  mutate(optim_alpha = future_pmap(list(comparisons, method, TP_mult, FP_mult), function(x, y, z, w) {
    optimal = optimize(alpha_optim, interval = c(0, 1), comparisons = x, method = y, TP_mult = z, FP_mult = w, maximum = TRUE)
    tibble(maximum = optimal$maximum, optimum = optimal$objective)
  })) %>%
  unnest(optim_alpha) %>%  
  mutate(method = ifelse(method == "none", "uncorrected", method))

plan(sequential)
```

Next, we expand our grid to include different values of $\alpha \in \{0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8\}$ and incentive schemes. We then calculate the number of TP, TN, FP and FN for each trial, at at each level of $\alpha$

```{r, fig.width = 12, fig.height = 6}
# what are the different FDRs that we want to control for?
alphas = c(0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8) 

data.sample.summary.alpha = data.sample.stats %>%
  ungroup() %>%
  filter(p_null == 0.8) %>%
  expand(
    nesting(region, mu, p, method, n_regions, p_null, trial), 
    alpha = alphas,
    nesting(TP_mult = c(1, 1, 1, 9), FP_mult = c(-19, -9, -1, -1))
  ) %>%
  group_by(method, n_regions, alpha, TP_mult, FP_mult, trial) %>%
  mutate(
    reject_null = p < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)
```

We can then calculate the average expected payout across the set of trials as a function of $\alpha$. We also highlight the points where expected payout is maximum based on our optimisation calculation previously.

```{r, fig.width = 12, fig.height = 6}
data.sample.summary.alpha %>%
  mutate(payout = TP*TP_mult + TN - FN + FP*FP_mult) %>%
  summarise(E_payout = mean(payout), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = alpha, y = E_payout, group = factor(n_regions), colour = n_regions)) +
  geom_point(data = df.alpha_optimal, aes(x = maximum, y = optimum), color = "red", size = 1) +
  facet_grid(interaction(TP_mult, FP_mult) ~ method, scales = "free_y") +
  scale_x_log10() +
  theme_minimal()
```
The figure above indicates that, when $p(null) = 0.8$, at each level # of comparison, the payout is maximised at values of $\alpha$ close to 0.05 only for the BH correction method. For the Bonferroni correction, the $\alpha$ value at which payout is maximum *increases* with increase in the # of possible comparisons, as Bonferroni is a very conservative correction that gives greater chance of FNs compared to the other methods. On the other hand, for the uncorrected strategy, the $\alpha$ value at which payout is maximum *decreases* with increase in the # of possible comparisons; this is because the uncorrected strategy admits a greater chance of FPs in multiple comparisons scenarios, and one should be more conservative in rejecting null hypotheses for optimal results.

## Comparisons with theoretical estimates

So far, we have performed simulations to understand how different statistical correction methods, significance levels for statistical tests and properties of the data generating process (such as $p(null)$) affect False Discovery Rates and Expected Payout under an incentive scheme.

### p-value PDF, CDF, invCDF definitionss

- Definitions of PDF and CDF of the $p$-value from [@hung_behavior_1997]
- Random draws from the $p$-value PDF `rp` is sampled through random draws from the uniform [0, 1] quantile space, then looked up through the inverse CDF function (using `uniroot`). 
- **Limitation**: These functions are dependent on the $\mu$, $\sigma$, $K$ parameters. These parameters are most likely different IRL but we are not using other values yet.

```{r}
K = sample_size

# PDF 
f_p <- function(x, mu, sigma, K) {
  dnorm(qnorm(1 - x) - sqrt(K) * mu / sigma) / dnorm(qnorm(1 - x))
}

# CDF
F_p <- function(x, mu, sigma, K) {
  1 - pnorm(qnorm(1 - x) - sqrt(K) * mu/sigma)
}


# inverse CDF of p-value
F_p_inv <- function(q, mu, sigma, K, l = 0, u = 1){
  uniroot(function(p) F_p(p, mu, sigma, K) - q, lower = l, upper = u)$root
}

# random sample from PDF of p-value using its invCDF
rp <- function(mu, sigma){
  q <- runif(1)
  F_p_inv(q, mu, sigma, K)
}
```

Next, we create a function which calculates the average false discovery rate and payout (across the set of trials):

```{r}
fp_penalty = 19
n_trials = 1000

calculate_payout = function(alpha, p_null) {
  expand_grid(nregions = n_region, iter = 1:n_trials, mu = 0.9, sigma = 1) %>%
    uncount(nregions, .remove = FALSE, .id = "panel") %>%
    mutate(unif = map_dbl(row_number(), ~ sobol(1, seed = ., scrambling = 2))) %>%
    group_by(nregions, iter) %>%
    mutate(
      mu = ifelse(unif > p_null, mu, 0),
      p_raw = map2_dbl(mu, sigma, ~rp(.x, .y)), 
      p_bh = p.adjust(p_raw, method = "BH"),
      p_bonf = p.adjust(p_raw, method = "bonferroni"),
    ) %>%
    pivot_longer(starts_with("p_"), names_to = "method", values_to = "p")  %>%
    mutate(
      true = mu == 0, # null hypothesis being true
      reject = p < alpha
    ) %>%
    group_by(iter, nregions, method) %>%
    summarize( tp = sum(!true & reject),
               fp = sum(true & reject),
               tn = sum(true & !reject),
               fn = sum(!true & !reject), 
               .groups = "drop_last") %>%
    mutate(fdr =  ifelse(tp + fp != 0, (fp) / (tp + fp), 0),
           pay = (tp - fp_penalty * fp + tn - fn) * 1) %>%
    group_by(method, nregions) %>%
    summarize(E_pay = mean(pay), E_fdr = mean(fdr), .groups = "drop")
}
```

We then create a grid of different $\alpha$ and $p(null)$ values which are passed as arguments to the `calculate_payout` function defined above. As this takes a long time to execute, we store it in a .rds file which we then load subsequently:

```{r}
plan(multisession, workers = 8)

data.sim.alpha_pnull = expand_grid(
    alpha = seq(0.01, 0.15, by = 0.01),
    p_null = seq(0.04, 1, by = 0.02)
  ) %>%
  mutate(sim = future_map2(alpha, p_null, calculate_payout, .options = furrr_options(seed = TRUE), .progress = TRUE)) %>%
  unnest(sim)

plan(sequential)

data.sim.alpha_pnull %>%
  saveRDS(file = "simulation-invCDF-alpha_pnull.rds")
```

The following plot shows the simulated results of payout as a function of alpha and p(null)

```{r, fig.width = 12, fig.height = 3}
data.sim.alpha_pnull = readRDS(file = "simulation-invCDF-alpha_pnull.rds")

data.sim.alpha_pnull %>%
  group_by(alpha, method, nregions) %>%
  summarise(value = mean(E_pay), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = alpha, y = value, colour = nregions, group = nregions)) +
  facet_grid(. ~ method) +
  theme_minimal()
```

```{r, fig.width = 12, fig.height = 3}
data.sim.alpha_pnull %>%
  filter(alpha == 0.05) %>%
  ggplot() +
  geom_line(aes(x = p_null, y = E_pay, colour = method)) +
  facet_grid(. ~ nregions, scales = "free_y") +
  theme_minimal()
```


