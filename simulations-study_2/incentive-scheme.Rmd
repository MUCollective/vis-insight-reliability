---
title: "Simulation for determining an incentive scheme"
author: "Abhraneel Sarma"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(future)
library(furrr)
```

# The problem 
The problem with the previous study design was that our incentives did not actually match the false discovery rate we were attempting to control for. Consider the Benjamini-Hochberg procedure for instance. To account for multiple comparisons, BH controls FDR at $\alpha < 0.05$ by modifying the significance region $\Gamma$ such that we reject hypothesis, $H_{0, i}$ if a given test-statistic, $t_i \in \Gamma$. Here, we explore a formulation of the loss function which will allow us to obtain the observed behavior.

## The average FDR for m hypothesis tests

From [@Soloff et al.]:

For $i = 1, ..., m$, let $H_i = 0$ if the i-th hypothesis is null and $H_i = 1$ otherwise, and consider the simple Bayesian two-groups model:

$$
p_i | H_i = h \overset{ind}{\sim} f_h \text{ with } H_i \overset{iid}{\sim}\text{Bern}(1 - \pi_0) \text{ for } i = 1, ..., m
$$
where $f_0$ and $f_1$ are the null and alternative densities respectively supported on the unit interval [0, 1], and the null proportion is $\pi_0 \in [0, 1]$.

Assumptions: 
the null density is uniform $f_0 = 1_{[0, 1]}$
$f := \pi_0 + (1 - \pi_0)f_1$ denotes the common mixture density of the p-values in the model.

## Determining an incentive scheme

Let the following be the possible outcomes from m hypothesis tests:

+-------------------+--------------------+----------------+-----------+
|                   | Do not reject null | Reject null    |   Total   |
+===================+====================+================+===========+
| Null true         |          U         |       V        |  $m_{0}$  |
+-------------------+--------------------+----------------+-----------+
| Alternative true  |          T         |       S        |  $m_{1}$  |
+-------------------+--------------------+----------------+-----------+
|                   |          W         |       R        |    m      |
+-------------------+--------------------+----------------+-----------+

We want an incentive structure such that participants in a multiple comparisons scenario which results in the optimal payoff when FDR is controlled at level $q$. The nominal value of $q$ used is usually 5%.

## Weighted Classification Loss

The total loss over $m$ instances ($m$ multiple comparisons) is:

$$
\begin{align*}
L &= c_1V + c_2(m_1 - (R-V)) \\
\end{align*}
$$

where, $c_1$ is the cost incurred from each False Discovery and $c_2$ is the cost incurred from each false non-discovery (as $m_1 - (R - V) = T$). Let $\lambda = c_1/c_2$, be the ratio between the two mis-classification losses.

$$
L = c_2m.L_{\lambda}(H, R) + c_2.m_1 \\
\text{where, }L_{\lambda} := \frac{(1 + \lambda)V - R}{m}
$$

here, $L_{\lambda}$ is the per-instance weighted classification loss. if $\pi_0$ is known, we can estimate V, which gives us the following estimator of the loss:

$$
\begin{align}
\hat{L}_{\lambda}(t; \pi_o) &= \frac{(1 + \lambda)m\pi_0t - mF_m(t)}{m} = (1 + \lambda)(\pi_0t - \alpha F_m(t)) \\
\text{where, } &F_m(t) \text{ is the empirical cumulative distribution function (ecdf) of the p-values.} \\
F_m(t) &:= \frac{1}{m} \sum_{i=1}^{m} 1\{p_i \leq t\} \\
\end{align}
$$

*Question: can we use the theoretical distribution of the p-value in place of the ecdf here?*
Answer: Likely, No. Because $R$ depends on the procedure used to correct for False Discoveries, this likely means that we cannot obtain a theoretical estimate. As such, we can do possibly one of two things:
- (attempt) a bounds analysis which will help us estimate the maximum possible loss
- use simulations

# Simulations
For now, simulations appear to be the simplest choice. We might go back to (1) later. The following data structure contains a grid over:
- number of comparisons, $m: \in \{1, 2, 4, 8, 16, 20, 50\}$.
- probability of the null being true, $\pi_0 \in \{0.1, 0.2, ..., 0.9\}$
with the following null hypothesis:

$$
\mu_i \overset{iid}\sim \begin{cases}
  H_0: 0 & \text{with probability } & \pi_0 \\
  H_1: \neq 0 & \text{with probability } & \pi_1 = (1 - \pi_0) \\
\end{cases}
$$
We use an effect size, $\delta = \mu / \sigma = 0.9$, following [@Kale et al.] to simulate the dataset. First, we load the simulated dataset, and count the number of TP, TN, FP and FN in each trial, over the simulated design space ($m$, $\pi_0$):

```{r, fig.width = 12, fig.height = 3}
data.sample.stats = readRDS("sim_data/sample_stats-n10000.rds")
head(data.sample.stats)

## remove some of the unnecessary columns
data.sample.stats = select(data.sample.stats, -effect_size)
```

Then, using $\alpha = 0.05$, we estimate the number of True Positives, True Negatives, False Positives and False Negatives, in a single trial (i.e. one scenario where a agent has to perform m NHSTs):

```{r, fig.width = 12, fig.height = 3}
alpha = 0.05

data.sample.stats.summary = data.sample.stats %>%
  group_by(method, m, p_null, trial) %>%
  mutate(
    reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision 
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum)

head(data.sample.stats.summary)
```

We then calculate and visualise the average False Positive and False Negative Rates (in 1000 trials) based on rejecting the p-value at $\alpha = 0.05$. We observe that, as expected, without correcting for multiple comparisons, we would expect the FDR > 0.05 (when $\pi_0 > 0.5$), while both the BH and Bonferroni multiple comparisons correction control for FDR. However, we also notice from the second plot that Bonferroni has a very high False Negative Rate.

```{r, fig.width = 16, fig.height = 6}
p1 = data.sample.stats.summary %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop") %>%
  ggplot() +
  geom_hline(aes(yintercept = alpha), color = "black") +
  geom_line(aes(x = p_null, y = FDR, color = method)) +
  facet_grid(. ~ m, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.2)) +
  labs(x = bquote(Pr(H[0]))) +
  theme_minimal()

p2 = data.sample.stats.summary %>%
  mutate(FNR = ifelse((FN + TN), FN / (FN + TN), 0)) %>%
  summarise(FNR = mean(FNR), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = FNR, color = method)) +
  facet_grid(. ~ m, scales = "free_y") +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.2)) +
  labs(x = bquote(Pr(H[0]))) +
  theme_minimal()

cowplot::plot_grid(p1, p2, ncol = 1, labels = c("False Positive/Discovery Rate", "False Negative Rate"), label_x = c(-0.07, -0.04), vjust = -0.1) +
  theme(plot.margin = margin(0.5, 0, 0, 0, "cm"))
```

Note that Benjamini-Hochberg actually controls for False Discovery Rate at $\alpha = \pi_0 q$ where q is generally set at 0.05. However, in most scenarios, $\pi_0$ is not known. In our case, we can see that it is true from the plot above, as the FDR from the Benjamini-Hochberg correction resembles a straight line through zero with slope 0.05.

From the plot below, we can verify the Benjamini-Hochberg is the only method which controls for False Discovery Rates (averaged over all values of $\pi_0$):

```{r, fig.width = 8, fig.height = 3}
data.sample.stats.summary %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop_last") %>%
  summarise(FDR = mean(FDR), .groups = "drop") %>%
  ggplot() +
  geom_vline(aes(xintercept = alpha/2), color = "black") +
  geom_point(aes(x = FDR, y = factor(m), color = method)) +
  theme_minimal()
```
The challenge we face is in identifying an incentive scheme such that participants would perform optimally if they control False Discoveries at $q \leq 0.05$. [@Soloff et al.] identify two loss functions (which are analogous to incentive schemes---namely, the per-instance weighted classification costs and the total classification costs over m instances. Next, we inspect whether, with either of thse loss formulations,  Benjamini-Hochberg has the lowest loss among the various correction methods. The plot below indicates that it does indeed have the lowest Loss by a considerable margin

```{r, fig.width = 16, fig.height = 3}
c1 = 19
c2 = 1

data.sample.stats.summary %>%
  mutate(
    Loss.total = c1*FP + c2*FN,
    Loss.piwc = (c1*FP - TP)/m,
    Loss.new = (c1*FP + c2*FN - (4*TP))/m,
  ) %>%
  summarise(Loss.piwc = mean(Loss.piwc), Loss.total = mean(Loss.total), Loss.new = mean(Loss.new), .groups = "drop_last") %>%
  summarise(Loss.piwc = mean(Loss.piwc), Loss.total = mean(Loss.total), Loss.new = mean(Loss.new), .groups = "drop") %>%
  pivot_longer(cols = starts_with("Loss"), names_prefix = "Loss.", names_to = "Type", values_to = "Loss") %>%
  ggplot() +
  geom_point(aes(x = Loss, y = factor(m), color = method)) +
  facet_wrap(~ Type, scales = "free") +
  theme_minimal()
```

As an aside, let's verify if BH controls for FDR at $\pi_0q$ where $q$ is the rate at which we are trying to control for FDR.


```{r, fig.width = 16, fig.height = 3}
data.sample.stats.summary %>%
  filter(p_null >= 0.5) %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop_last") %>%
  ggplot() +
  geom_vline(aes(xintercept = alpha*p_null), color = "black") +
  geom_vline(aes(xintercept = alpha), color = "red") +
  geom_vline(aes(xintercept = alpha*p_null/(1 - p_null*(1 - alpha))), color = "magenta") +
  geom_point(aes(x = FDR, y = factor(m), color = method)) +
  facet_grid(. ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()
```

While it is clear from the previous plots that BH is actually controlling for FDR as expected, we do notice something surprising---that the FDR for the uncorrected strategy appears to have an asymptotic upper bound given by $\frac{\alpha\pi_0}{(1 - \alpha)(1 - \pi_0)}$. To better understand this, let us explore the total number of False Positives and False Negatives. We also compute the expected number of False Positives ($\alpha \pi_0 m$) and True Positives ($(1 - \pi_0) m$).

```{r, fig.width = 16, fig.height = 5}
E_outcomes = crossing(
  m = c(1, 2, 4, 8, 12, 16, 20, 50), 
  p_null = seq(0.5, 0.9, by = 0.1)
) %>% mutate(E_fp = p_null*m*alpha, E_tp = (1 - p_null)*m)

p1 = data.sample.stats.summary %>%
  mutate(p_null = as.integer(p_null*10)/10) %>%
  filter(p_null >= 0.5) %>%
  summarise(FP = mean(FP), .groups = "drop_last") %>%
  ggplot() +
  geom_point(aes(x = FP, y = factor(m), colour = method)) +
  geom_line(aes(x = FP, y = factor(m), group = method, colour = method)) +
  geom_line(data = E_outcomes, aes(x = E_fp, y = factor(m), group = 1), alpha = 0.5) +
  facet_grid(. ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()

p2 = data.sample.stats.summary %>%
  mutate(p_null = as.integer(p_null*10)/10) %>%
  filter(p_null >= 0.5) %>%
  summarise(TP = mean(TP), .groups = "drop_last") %>%
  ggplot() +
  geom_point(aes(x = TP, y = factor(m), colour = method)) +
  geom_line(aes(x = TP, y = factor(m), group = method, colour = method)) +
  geom_line(data = E_outcomes, aes(x = E_tp, y = factor(m), group = 1), alpha = 0.5) +
  facet_grid(. ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()

cowplot::plot_grid(p1, p2, ncol = 1)
```

Here, note that the number of TP for the Bonferroni strategy is very close to the strategy used by the others. This implies that, for the current value of $\delta$ used for this particular simulated dataset, we would obtain the same number of TP even if we used a smaller value of $\alpha$ as our rejection criteria. I suspect that the upper bound will likely exist for sufficiently large values of $\delta$. Let's explore alternative values of $\delta$ 

## Varying effect size of the simulated data

```{r, eval = FALSE}
alpha = 0.05

data.sample.stats.delta = readRDS("sim_data/sample_stats-delta-n1e4.rds")

data.sample.stats.summary.delta = data.sample.stats.delta %>%
  group_by(method, m, p_null, effect_size, trial) %>%
  mutate(
    reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision 
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  summarise_at(.vars = vars(TP, FP, TN, FN), sum) %>%
  mutate(
    p_null = as.integer(p_null*10)/10,
    effect_size = as.integer(effect_size*10)/10
  )

rm(data.sample.stats.delta)
saveRDS(data.sample.stats.summary.delta, "sim_data/sample_stats_summary-delta-n1e4.rds")
```


```{r, fig.width = 16, fig.height = 12}
data.sample.stats.summary.delta = readRDS("sim_data/sample_stats_summary-delta-n1e4.rds")

E_outcomes = crossing(
  m = c(1, 2, 4, 8, 12, 16, 20, 50), 
  p_null = seq(0.5, 0.9, by = 0.1)
) %>% mutate(E_fp = p_null*m*alpha, E_tp = (1 - p_null)*m)

p1.delta = data.sample.stats.summary.delta %>%
  summarise(FP = mean(FP), .groups = "drop_last") %>%
  ggplot() +
  geom_vline(aes(xintercept = alpha*p_null), color = "black") +
  geom_vline(aes(xintercept = alpha), color = "red") +
  geom_point(aes(x = FP, y = factor(m), colour = method)) +
  geom_line(aes(x = FP, y = factor(m), group = method, colour = method)) +
  geom_line(data = E_outcomes, aes(x = E_fp, y = factor(m), group = 1), alpha = 0.5) +
  facet_grid(effect_size ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()

p2.delta = data.sample.stats.summary.delta %>%
  summarise(TP = mean(TP), .groups = "drop_last") %>%
  ggplot() +
  geom_point(aes(x = TP, y = factor(m), colour = method)) +
  geom_line(aes(x = TP, y = factor(m), group = method, colour = method)) +
  geom_line(data = E_outcomes, aes(x = E_tp, y = factor(m), group = 1), alpha = 0.5) +
  facet_grid(effect_size ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()

cowplot::plot_grid(p1.delta, p2.delta, ncol = 1)
```


```{r, fig.width = 16, fig.height = 9}
data.sample.stats.summary.delta %>%
  mutate(FDR = ifelse((FP + TP), FP / (FP + TP), 0)) %>%
  summarise(FDR = mean(FDR), .groups = "drop_last") %>%
  ggplot() +
  geom_vline(aes(xintercept = alpha*p_null), color = "black") +
  geom_vline(aes(xintercept = alpha), color = "red") +
  geom_vline(aes(xintercept = alpha*p_null/(1 - p_null*(1 - alpha))), color = "magenta") +
  geom_point(aes(x = FDR, y = factor(m), color = method)) +
  facet_grid(effect_size ~ p_null) +
  labs(y = "m (number of comparisions") +
  theme_minimal()
```

Based on the plots above, it seems like if effect size $\delta > 0.5$ or $\pi_0 < 0.5$, we are likely to see FDR values converging to a upper bound which is estimated by the magenta line. This upper bound implies that if we adjust the value of $\alpha$, we would be controlling for FDR at a particular value. Moreover, it does not make a lot of sense t include values of $\pi_0 > 0.7$. Thus, we will only consider this sub-space of the variables $\delta$ and $\pi_0$.


## Loss function

We explore the behavior of the three different correction strategies for two different formulations of the loss functions (per-instance weighted classification loss and total classification loss):

First, using the per-instance weighted classification loss $L = c_1V + c_2(m_1 - (R-V))$, we can estimate the loss incurred for a given set of costs c1 and c2 by each of these procedures:

```{r, fig.width = 16, fig.height = 6}
c1 = 19
c2 = 1

data.sample.stats.summary.delta %>%
  mutate(Loss = (c1*FP + c2*FN - (m - (FP + FN)))/m) %>%
  summarise(Loss = mean(Loss), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = Loss, color = method)) +
  facet_grid(effect_size ~ m, scales = "free_y") +
  labs(x = bquote(Pr(H[0]))) +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.1)) +
  theme_minimal()
```

We notice that, at smaller values of $m$, the behavior of the loss function is less well-defined. (This is likely because, for small $m$, the values of FDR for each trial are in smaller bins---i.e. let $m = 2$, the possible values of FDR are 0, 0.5 and 1). Thus, let's look at larger values of $m$, specifically $m = 20$.

Next, we look at the total classification loss

```{r, fig.width = 16, fig.height = 6}
c1 = 9
c2 = 1

data.sample.stats.summary.delta %>%
  mutate(Loss = (c1*FP + c2*FN)/m) %>%
  summarise(Loss = mean(Loss), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = Loss, color = method)) +
  facet_grid(effect_size ~ m, scales = "free_y") +
  labs(x = bquote(Pr(H[0]))) +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.1)) +
  theme_minimal()
```
### Loss function criteria

Let us identify a set of criteria we want our Loss function to satisfy. 
We want to ensure that, given a particular **loss function**, people should be incentivised to perform multiple comparisons correction if $m > 1$. If people use an uncorrected strategy, they will perform worse than a corrected strategy. In other words, when $m > 1$, we would expect a multiple comparisons correction strategy such as BH to yield a higher payout than the other strategies. This can be formulated as follows:

$$
\mathrm{Payout}(x, strategy, m): \mathrm{Payout}(x | strategy \in {BH}) > \mathrm{Payout}(x | strategy \in \{\mathrm{uncorrected, bonferroni}\})
$$

Although the above criteria is a good starting point, it omits important considerations such as the value of $\alpha$ at which we are controlling False Discovery Rates. Below, we formulate a better criterion:

Our loss function is considered admissible if, *at a particular value of* $\alpha = \alpha_{m = 1}$ which optimises payout in the scenario without multiple comparisions correction, the expected payout using the BH strategy dominates the uncorrected (and Bonferroni) strategy for all multiple comparision scenarios. We formulate this as below:

$$
\exists \alpha_{m=1} s.t. E(payout | m, strategy = BH, \alpha_{BH}) >  E(payout | m, strategy = uncorrected, \alpha = \alpha_{m=1})  \forall m > 1 \\
\text{where } \alpha_{m = 1} = \mathrm{argmax}_\alpha E(payout | m = 1)
$$

$$
\exists \alpha_{m=1}, \pi_0, \delta s.t. E(payout | m, strategy = BH, \alpha_{m = 1}, \pi_0, \delta) >  E(payout | m, strategy = uncorrected, \alpha_{m=1}, \pi_0, \delta)  \forall m > 1 \\
\text{where } \alpha_{m = 1} = \mathrm{argmax}_\alpha E(payout | m = 1, \alpha, \pi_0, \delta)
$$
Note: if we want 19 TPs for every 1 FP, does $\mathrm{argmax}_\alpha E(payout | m = 1, \alpha, \pi_0, \delta) = 0.05$? If not, what should be the expected payout 

### Validating the criteria

```{r}
data.sample.stats.delta = readRDS("sim_data/sample_stats-delta-n1e4.rds")
data.sample.stats.delta.m1 = readRDS("sim_data/sample_stats-m1-delta-n1e5.rds") %>%
  rename(p.value = p_uncorrected)
```

```{r, fig.width = 12, fig.height = 4}
data.sample.stats.delta.m1 %>%
  mutate(
    alpha = 0.05,
    reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  ) %>%
  group_by(p_null, effect_size) %>%
  mutate(Loss = loss.piwc(TP, TN, FP, FN, m, 19)) %>%
  summarise(Loss = mean(Loss), .groups = "drop_last") %>%
  ggplot() +
  geom_point(aes(x = Loss, y = factor(effect_size))) +
  facet_grid(. ~ p_null) +
  theme_minimal()
```


Now, we will calculate the value of $\alpha$ which minimises payoff for $m = 1$. We will be using the `optimise` function in R to perform this computation. Because this step is computationally intensive, we break the data frame `data.sample.stats.delta` into multiple smaller data frames containing a single value of the variables `p_null` and `effect_size`. Prior to actually implementing this step, we need to verify whether our calculation workflow is correct. Thus, we first perform all the computations for a particular value of $\Pr(H_0) = 0.5$ and $\delta = 0.5$:

```{r}
dat.opt_group = data.sample.stats.delta.m1 %>% 
  filter(p_null == 0.5 & effect_size == 0.5) %>%
  nest() %>%
  ungroup()

dat.opt_group.stats = dat.opt_group %>%
  unnest(data) %>%
  group_by(trial) %>%
  mutate(
    alpha = alpha,
    reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
    TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
    FP = as.integer(!`mu > 0` & reject_null),
    TN = as.integer(!`mu > 0` & !reject_null),
    FN = as.integer(`mu > 0` & !reject_null)
  )

head(dat.opt_group.stats)
```

Next, we formulate our two loss functions as R functions, and apply them to the data structure defined previously:

```{r}
### Loss functions

## per-instance weighted classification
loss.piwc = function(tp, tn, fp, fn, c) {
  # ((1 + lambda)V - R) / m
  # lambda = c1/c2; c1 = c and c2 = 1 => lambda = c
  # V: # of fp
  # R: total # of rejections => tp + fp
  # m: 1
  ((1 + c)*fp - (fp + tp))
}

## total loss over m instances
loss.total = function(tp, tn, fp, fn, c) {
  c*fp + fn
}

dat.opt_group.stats %>%
  mutate(cost = list(c(1, 2, 3, 4, 5, 9, 19))) %>%
  unnest(cost) %>%
  mutate(
    Loss = loss.piwc(TP, TN, FP, FN, cost)
  ) %>%
  group_by(cost) %>%
  summarise(Loss = mean(Loss), .groups = "drop_last") %>%
  mutate(squaredLoss = Loss^2)
```

We verify that the result from this calculation step is the same as the output of `alpha_optim`:

```{r}
alpha_optim = function(alpha, data, fun, ...) {
  data %>%
    mutate(alpha = alpha, m = 1) %>%
    mutate(
      reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
      TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
      FP = as.integer(!`mu > 0` & reject_null),
      TN = as.integer(!`mu > 0` & !reject_null),
      FN = as.integer(`mu > 0` & !reject_null)
    ) %>%
    mutate(Loss = fun(TP, TN, FP, FN, ...)) %>%
    mutate(Loss = Loss^2) %>% # squared loss here because @Soloff et al. we want to determine the break-even tolerance
    summarise(Loss = mean(Loss), .groups = "drop_last") %>%
    magrittr::extract2("Loss")
}

alpha_optim(0.05, dat.opt_group$data[[1]], loss.piwc, 19)
```

```{r, fig.height = 3, fig.width = 12}
alpha_optim2 = function(alpha, data, fun, ...) {
  data %>%
    mutate(alpha = alpha, m = 1) %>%
    mutate(
      reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
      TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
      FP = as.integer(!`mu > 0` & reject_null),
      TN = as.integer(!`mu > 0` & !reject_null),
      FN = as.integer(`mu > 0` & !reject_null)
    ) %>%
    mutate(Loss = fun(TP, TN, FP, FN, ...)) %>%
    summarise(Loss = mean(Loss), .groups = "drop_last") %>%
    magrittr::extract2("Loss")
}

plan(multisession, workers = 6)
data.loss.piwc = data.sample.stats.delta.m1 %>%
  mutate(cost = list(c(1, 2, 3, 4, 5, 9, 19))) %>%
  unnest(cost) %>%
  group_by(cost, effect_size, p_null) %>%
  nest() %>%
  group_split() %>%
  future_map_dfr(
    ~ mutate(., Loss = map2(data, cost, ~ optimise(alpha_optim2, interval = c(0, 1), .x, loss.piwc, .y))),
    .progress = TRUE
  ) %>%
  select(-data) %>%
  unnest_wider(Loss)
plan(sequential)

plain <- function(x,...) {
  format(x, ..., scientific = FALSE, drop0trailing = TRUE)
}

data.loss.piwc %>%
  # Loss: ((1 + c)*fp - (fp + tp)); c = 19
  ggplot() +
  geom_point(aes(x = minimum, y = factor(effect_size), colour = factor(cost))) +
  geom_path(aes(x = minimum, y = factor(effect_size), colour = factor(cost), group = factor(cost))) +
  geom_vline(aes(xintercept = 0.05)) +
  facet_grid( ~ p_null) +
  scale_x_log10(labels = plain) +
  # scale_x_continuous(breaks = seq(0, 0.06, by = 0.01), limits = c(0, 0.06)) +
  labs(x = "alpha", y = "effect size") +
  theme_minimal()
```

We put all the previous steps together to calculate the value of alpha that minimises loss for each value of $m, \pi_0, \text{ and, } strategy$, as well as the corresponding value of loss using a given loss function.

```{r, fig.height = 3, fig.width = 12}
plan(multisession, workers = 6)
data.loss.piwc = data.sample.stats.delta.m1 %>%
  mutate(cost = list(c(1, 2, 3, 4, 5, 9, 19))) %>%
  unnest(cost) %>% 
  group_by(cost, effect_size, p_null) %>%
  nest() %>%
  group_split() %>%
  future_map_dfr(
    ~ mutate(., Loss = map2(data, cost, ~ optimise(alpha_optim, interval = c(0, 1), .x, loss.piwc, .y))),
    .progress = TRUE
  ) %>%
  select(-data) %>%
  unnest_wider(Loss)
plan(sequential)

plain <- function(x,...) {
  format(x, ..., scientific = FALSE, drop0trailing = TRUE)
}

data.loss.piwc %>%
  ggplot() +
  geom_point(aes(x = minimum, y = factor(effect_size), colour = factor(cost))) +
  geom_line(aes(x = minimum, y = factor(effect_size), colour = factor(cost), group = factor(cost))) +
  facet_grid( ~ p_null) +
  scale_x_log10(labels = plain) +
  geom_vline(xintercept = 0.05) +
  # scale_x_continuous(breaks = seq(0, 0.06, by = 0.01), limits = c(0, 0.06)) +
  labs(x = "alpha", y = "effect size") +
  theme_minimal()
```

Based on the plot above, it seems like if the cost-ratio for false positives to false negatives is 19, we observe that per-instance weighted classification loss breaks even at $\alpha = 0.05$ when $Pr(H_0) = 0.5$ and $\delta \in {0.5, 0.7}$.

```{r, fig.width = 16, fig.height = 7}
data.sample.stats.summary.delta = readRDS("sim_data/sample_stats_summary-delta-n1e4.rds")
```

#### Per-instance weighted classification loss

```{r, fig.width = 16, fig.height = 7}
c1 = 19
data.sample.stats.summary.delta %>%
  mutate(Loss = ((1 + c1)*FP - (FP + TP))/m) %>%
  summarise(Loss = mean(Loss), .groups = "drop_last") %>%
  mutate(p_null = paste0("Pr(H_0): ", p_null), effect_size = paste0(expression(delta), ":", effect_size)) %>%
  ggplot() +
  geom_point(aes(x = Loss, y = factor(m), color = method)) +
  facet_grid(effect_size ~ p_null, labeller = label_parsed, scales = "free_x") + 
  labs(y = "m (number of comparisions)") +
  theme_minimal()
```

```{r, fig.width = 16, fig.height = 7}
c1 = 5
data.sample.stats.summary.delta %>%
  mutate(Loss = ((1 + c1)*FP - (FP + TP))/m) %>%
  summarise(Loss = mean(Loss), .groups = "drop_last") %>%
  mutate(p_null = paste0("Pr(H_0): ", p_null), effect_size = paste0(expression(delta), ":", effect_size)) %>%
  ggplot() +
  geom_point(aes(x = Loss, y = factor(m), color = method)) +
  facet_grid(effect_size ~ p_null, labeller = label_parsed, scales = "free_x") + 
  labs(y = "m (number of comparisions)") +
  theme_minimal()
```

#### Total classification loss

```{r, fig.width = 16, fig.height = 7}
c1 = 19
data.sample.stats.summary.delta %>%
  mutate(Loss = (c1*FP + FN)/m) %>%
  summarise(Loss = mean(Loss), .groups = "drop_last") %>%
  mutate(p_null = paste0("Pr(H_0): ", p_null), effect_size = paste0(expression(delta), ":", effect_size)) %>%
  ggplot() +
  geom_point(aes(x = Loss, y = factor(m), color = method)) +
  facet_grid(effect_size ~ p_null, labeller = label_parsed) + 
  labs(y = "m (number of comparisions)") +
  theme_minimal()
```



What if we instead calculate the cost which would result in the minimum Loss at $\alpha = 0.05$? To do that, we define an alternate optimisation function, `cost_optim` which should minimise loss at $\alpha = 0.05$

```{r}
cost_optim = function(cost, data, fun) {
  data %>%
    mutate(alpha = 0.05) %>%
    mutate(
      reject_null = p.value < alpha, `mu > 0` = mu != 0, # reject or not?
      TP = as.integer(`mu > 0` & reject_null), # correctness of the decision
      FP = as.integer(!`mu > 0` & reject_null),
      TN = as.integer(!`mu > 0` & !reject_null),
      FN = as.integer(`mu > 0` & !reject_null)
    ) %>%
    mutate(Loss = fun(TP, TN, FP, FN, cost)) %>%
    summarise(Loss = mean(Loss), .groups = "drop_last") %>%
    mutate(Loss = Loss^2) %>% # squared loss here because @Soloff et al. we want to determine the break-even tolerance
    magrittr::extract2("Loss")
}

cost_optim(19, dat.opt_group$data[[1]], loss.piwc)
```


```{r, fig.height = 3, fig.width = 12}
plan(multisession, workers = 5)
data.loss.piwc.cost = data.sample.stats.delta.m1 %>% 
  group_by(effect_size, p_null) %>%
  nest() %>%
  group_split() %>%
  future_map_dfr(
    ~ mutate(., Loss = map(data, ~ optimise(cost_optim, interval = c(1, 20), ., loss.piwc.m1))),
    .progress = TRUE
  ) %>%
  select(-data) %>%
  unnest_wider(Loss)
plan(sequential)

data.loss.piwc.cost %>%
  ggplot() +
  geom_point(aes(x = minimum, y = factor(effect_size))) +
  geom_vline(xintercept = 19, colour = "red") +
  facet_grid(. ~ p_null) +
  theme_minimal()
```



*Verify the theoretical calculation of the loss function using Wolfram Alpha*


### Ignore whatever's below this...


If we can, then

$$
\begin{align}
F_m &= \alpha \pi_0 + (1 - \pi_0)G_{\delta}(p_i \leq t) \\
\text{where, } G_{\delta}(p) &= 1 - \Phi(Z_p - \sqrt n\mu/\sigma) \\
\end{align}
$$


The function $G_{\delta}(p) can be implemented as:

```{r}
# CDF
G_p <- function(x, delta = 0.9, K = 20) {
  1 - pnorm(Z_p(x) - sqrt(K) * delta) # 1 - \Phi(Z_p - sqrt(n) \delta)
}

# PDF 
f_p <- function(x, delta = 0.9, K = 20) {
  dnorm(Z_p(x) - sqrt(K) * delta) / dnorm(Z_p(x)) # \phi(Z_p - sqrt(n)\delta) / \phi(Z_p)
}

Z_p = function(p) {
  qnorm(1 - p)  # 1 - \Phi(Z_p - sqrt(n) \delta)
}

G_p(0.05, 0.9, 20)

E_delta_p = function(delta = 0.9, K = 20) {
  integrand = function(v) pnorm(-v) * dnorm(v - sqrt(K) * delta)
  integrate(integrand, -Inf, Inf)
}

E_delta_p()
```


```{r}
pi0 = 0.7
E_TP = (1 - pi0)*G_p(0.05, 0.5, 20)
E_TN = pi0*(1 - 0.05)
E_FP = pi0*0.05
E_FN = (1 - pi0)*(1 - G_p(0.05, 0.5, 20))

paste(paste0("E(TP): ", round(E_TP, 2)), paste0("E(TN): ", round(E_TN, 2)), paste0("E(FP): ", round(E_FP, 2)), paste0("E(FN): ", round(E_FN, 2)), sep = "\n") %>%
  cat()
```


```{r, fig.width = 16, fig.height = 3}
data.sample.stats.summary %>%
  mutate(Loss = 9*FN + 22*FP - (TN + 3*TP)) %>%
  summarise(Loss = mean(Loss), .groups = "drop") %>%
  ggplot() +
  geom_line(aes(x = p_null, y = Loss, color = method)) +
  facet_grid(. ~ m, scales = "free_y") +
  labs(x = bquote(Pr(H[0]))) +
  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.2)) +
  theme_minimal()
```

Let's assume that $\pi_0 = 0.5$. Our data generating process for the stimuli assumes an effect size ($\delta$) of 0.9 This gives us the following value for $F_m := 0.05 \times 0.5 + 0.5 \times 0.5 \times 0.9913453 = 0.5206726$. Thus average Loss over m hypothesis tests would be:

```{r}
alpha = 0.05

Loss = alpha*p0 + (1 - p0)*G_p(alpha, effect_size, N)
```


We would like to compute the Expectation of Loss ($L_{\lambda}$) in a situation where an analyst performs m hypothesis tests:


The quantile of the standard Gaussian distribution is: 

$$
\begin{align*}
F^{-1}(p) &= \mu + \sigma \sqrt{2} \mathrm{erf}^{-1}(2p - 1) \\
&= \sqrt{2} \mathrm{erf}^{-1}(2p - 1)
\end{align*}
$$

Thus, 

$$
Z_p = F^{-1}(1 - p) = \sqrt{2} \mathrm{erf}^{-1}(2(1 - p) - 1)
$$


Putting it all together, the closed form of the CDF of the p-value is:

$$
G_{\delta}(p) = 1 - \Phi{(\sqrt{2} \mathrm{erf}^{-1}(2p - 1) - \sqrt{n}\mu/\sigma)}
$$

The Expectation of the p-value for a fixed $\delta$ is (\url{https://www.wolframalpha.com/input?i=%5Cint_%7B-infty%7D%5E%7Binfty%7D+%7B0.5%5B1+%2B+erf%28-x%2F%5Csqrt%7B2%7D%29%5D+%7D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D+e%5E%28-0.5%28x+-+%5Csqrt%7B20%7D*0.9%29%5E2%29+}):

$$
\begin{align*}
E_{\delta}(p) 
&= \int_{0}^{1} \frac{p\phi(Z_p - \sqrt{n}\mu/\sigma)}{\phi(Z_p)}dp \\
&= \int_{-\infty}^{\infty}\Phi(-v) \phi(v - \sqrt{n}\mu/\sigma)dv \\
&= \int_{-\infty}^{\infty}\frac{1}{2} \left[ 1 + \mathrm{erf}(\frac{-v}{\sqrt{2}}) \right] \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}(v - \sqrt{n}\mu/\sigma)^2}dv \\
\text{we have, } \delta &= \mu/\sigma = 0.9, n = 20 \\
\implies E_{\delta}(p)&= \int_{-\infty}^{\infty} \left[ 1 + \mathrm{erf}(\frac{-v}{\sqrt{2}}) \right] \frac{1}{\sqrt{2\pi}}e^{\frac{1}{2}(v - 0.9\sqrt{20})^2}dv \\
&= 0.00221326
\end{align*}
$$

Thus, the Expectation of our Loss function is:

$$
\begin{align*}
\hat{L}_{\lambda}(t; \pi_o) &= \frac{(1 + \lambda)m\pi_0t - mF_m(t)}{m} = (1 + \lambda)(\pi_0t - \alpha F_m(t)) \\
\end{align*}
$$





The challenge with the above loss function is that it relies on the distribution of the p-value under the alternative hypothesis. Unlike the distribution of the p-value under the null (which has a uniform distribution over the interval [0,1]), this distribution ($F_m$), is a function of both sample size and the true value or range of true values of the tested parameter [@Hung1997]. As such, we can do possibly one of three things:
- a bounds analysis which will help us estimate the maximum possible loss
- possibly integrate over the function $F_m$ over all possible values of $Z_p$ in equation (3) above (since the other parameters in the equation are fixed)
- use simulations

Let $I(\pi_0, \mathscr{R}_{(q)})$ be an incentive scheme where:
- $\pi_0$ is the prior probability of the null hypothesis being true
- $\mathscr{R}_{(q)}$ is the rejection criteria used which controls FDR at level $q$.

Let the result of this Rejection criteria be such that we get R rejections m - R non rejections. Thus, $FDR = V/R$ and $FNR = T/W$. Let us explore the desired properties of any formulation of the function $I$ at $\pi_0 \in {0, 0.5, 1}$.

$$
\begin{align*}
\text{if } \pi_0 = 0 &\implies \text{FDR} = 0, \text{FNR} \in [0, 1] \\
&\implies u = (W - T) - \omega T \\
\text{if } \pi_0 = 1 &\implies \text{FDR} \in [0, 1], \text{FNR} = 0 \\
&\implies u = (R - V) - \lambda V \\
\text{if } \pi_0 = 0.5 &\implies \text{FDR} \in [0, 1], \text{FNR} \in [0, 1] \\
&\implies u = ((R - V) - \lambda V) + ((W-T) - \omega T)
\end{align*}
$$

We want the following properties to hold for any formulation of the function $I$:

$$
\begin{align*}
\text{if } R = 0 \land \pi_0 < 1 &\implies I < 0 \\
&\implies I \propto \frac{1}{\pi_0}
\end{align*}
$$



There appears to be several different ways to control for FDR. We examine two:

1. Control the average FDR at $q = 5%$ using a procedure such as the Benjamini-Hochberg. The average FDR is given by:

$$
\mathrm{FDR} = E(\frac{V}{R}|R>0)Pr(R>0)
$$

2. Control the local false discovery rate (the likelihood that an individual discovery is a false lead)

$$
lfdr(t) := \mathbb{P}\{H_i = 0 | p_i = t\} = \frac{\pi_0}{f(t)}
$$

If using (2), we should only reject those hypotheses whose lfdr is below the break-even threshold of our cost-benefit tradeoff. Let $\lambda > 0$ define the ratio between the cost of each false discovery and the benefit of each true discovery. Then the utility of making $R$ rejections, of which $V$ are false discoveries is proportional to $(R - V) - \lambda V$. It follows that we should reject the i-th hypothesis if and only if $lfdr(p_i) \leq \alpha := \frac{1}{1+\lambda}$.

However, lfdr can only be computed if the p-values, $p_i = t$ are known. 



$$
R^{BH}_q := max\{k \in {0, ..., m} : p_k \leq \frac{qk}{m} \}
$$
where, $q$ is the level at which you wish to control false discovery rates

By [@Storey2003]

$$
\begin{align*}
\mathrm{FDR}(\Gamma) &= E[\frac{V(\Gamma)}{R(\Gamma)}|R>0].Pr(R > 0), \\
\\
&= Pr(H = 0 | T \in \Gamma).Pr(R > 0) \\
&=\frac{Pr(H = 0).Pr(T \in \Gamma | H = 0)}{Pr(H = 0).Pr(T \in \Gamma | H = 0) + Pr(H = 1).Pr(T \in \Gamma | H = 1)}.Pr(R > 0) \\
&= \frac{\pi_0.m.\alpha}{\pi_0.\alpha + (1 - \pi_0) . Pr(\text{p-value} < \alpha | H = 1)}.Pr(R > 0) \\
&= \frac{\pi_0.m.\alpha}{\pi_0.\alpha + (1 - \pi_0) . G_\delta(\alpha | H = 1)}.Pr(R > 0) \\
&= \frac{\pi_0.m.\alpha}{\pi_0.\alpha + (1 - \pi_0) . G_\delta(\alpha | H = 1)}.f(m) \\
f(m) &= \pi_0.(1 - \alpha)^m + (1 - \pi_0).G_\delta(\alpha | H = 1)^m
\end{align*}
$$


